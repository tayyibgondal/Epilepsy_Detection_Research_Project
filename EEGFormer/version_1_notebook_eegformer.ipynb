{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eIZuaC_C-UIX",
        "outputId": "389b3327-bfaf-4d75-edb3-dce32b13d2ab"
      },
      "outputs": [],
      "source": [
        "# !pip install mne\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import mne\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "B = 3  # batch size\n",
        "S = 21\n",
        "C = 120\n",
        "L = 150\n",
        "M = L//5  # reduced temporal dimension\n",
        "num_heads = 5\n",
        "device='cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-8ehN2De-UIc"
      },
      "outputs": [],
      "source": [
        "def find_files_by_extension(search_pattern):\n",
        "    edf_files = glob.glob(search_pattern, recursive=True)\n",
        "    return edf_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mOsapmWs-UId",
        "outputId": "69c0f2ba-44cb-485c-9d6f-9ef7b1345932"
      },
      "outputs": [],
      "source": [
        "search_pattern = pattern = r\"C:\\Users\\DELL\\Downloads\\tukl\\Implementations\\eegformer\\dataset_s\\*\\*\\*.edf\"\n",
        "file_paths = find_files_by_extension(search_pattern)\n",
        "\n",
        "for file_path in file_paths[:9]:\n",
        "    print(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hBd4_YSe-UIf",
        "outputId": "ecf2350d-6d7d-4508-bd59-a70d481ab57b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting EDF parameters from /home/arooba/ssd/hira/eegformer_try_2/C:\\Users\\DELL\\Downloads\\tukl\\Implementations\\eegformer\\dataset_s\\abnormal\\eval\\0000036.edf...\n",
            "EDF file detected\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/arooba/ssd/hira/eegformer_try_2/C:\\\\Users\\\\DELL\\\\Downloads\\\\tukl\\\\Implementations\\\\eegformer\\\\dataset_s\\\\abnormal\\\\eval\\\\0000036.edf'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmne\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Open the EDF file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# edf_file = mne.io.read_raw_edf(r'/content/0000002.edf')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m edf_file \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mread_raw_edf(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDELL\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDownloads\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtukl\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mImplementations\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39meegformer\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdataset_s\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mabnormal\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39meval\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m0000036.edf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Print basic information about the file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(edf_file\u001b[39m.\u001b[39minfo)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:1171\u001b[0m, in \u001b[0;36mread_raw_edf\u001b[0;34m(input_fname, montage, eog, misc, stim_channel, exclude, preload, verbose)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[39melif\u001b[39;00m ext \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39medf\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbdf\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOnly EDF and BDF files are supported, got \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1170\u001b[0m                               \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ext))\n\u001b[0;32m-> 1171\u001b[0m \u001b[39mreturn\u001b[39;00m RawEDF(input_fname\u001b[39m=\u001b[39;49minput_fname, montage\u001b[39m=\u001b[39;49mmontage, eog\u001b[39m=\u001b[39;49meog, misc\u001b[39m=\u001b[39;49mmisc,\n\u001b[1;32m   1172\u001b[0m               stim_channel\u001b[39m=\u001b[39;49mstim_channel, exclude\u001b[39m=\u001b[39;49mexclude, preload\u001b[39m=\u001b[39;49mpreload,\n\u001b[1;32m   1173\u001b[0m               verbose\u001b[39m=\u001b[39;49mverbose)\n",
            "File \u001b[0;32m</home/arooba/.local/lib/python3.10/site-packages/mne/externals/decorator.py:decorator-gen-157>:2\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, input_fname, montage, eog, misc, stim_channel, exclude, preload, verbose)\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/utils/_logging.py:90\u001b[0m, in \u001b[0;36mverbose.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[39mwith\u001b[39;00m use_log_level(verbose_level):\n\u001b[1;32m     89\u001b[0m         \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:115\u001b[0m, in \u001b[0;36mRawEDF.__init__\u001b[0;34m(self, input_fname, montage, eog, misc, stim_channel, exclude, preload, verbose)\u001b[0m\n\u001b[1;32m    113\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mExtracting EDF parameters from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(input_fname))\n\u001b[1;32m    114\u001b[0m input_fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(input_fname)\n\u001b[0;32m--> 115\u001b[0m info, edf_info, orig_units \u001b[39m=\u001b[39m _get_info(input_fname,\n\u001b[1;32m    116\u001b[0m                                        stim_channel, eog, misc,\n\u001b[1;32m    117\u001b[0m                                        exclude, preload)\n\u001b[1;32m    118\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mCreating raw.info structure...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[39m# Raw attributes\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:386\u001b[0m, in \u001b[0;36m_get_info\u001b[0;34m(fname, stim_channel, eog, misc, exclude, preload)\u001b[0m\n\u001b[1;32m    383\u001b[0m eog \u001b[39m=\u001b[39m eog \u001b[39mif\u001b[39;00m eog \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    384\u001b[0m misc \u001b[39m=\u001b[39m misc \u001b[39mif\u001b[39;00m misc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m []\n\u001b[0;32m--> 386\u001b[0m edf_info, orig_units \u001b[39m=\u001b[39m _read_header(fname, exclude)\n\u001b[1;32m    388\u001b[0m \u001b[39m# XXX: `tal_ch_names` to pass to `_check_stim_channel` should be computed\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m#      from `edf_info['ch_names']` and `edf_info['tal_idx']` but 'tal_idx'\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m#      contains stim channels that are not TAL.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m stim_ch_idxs, stim_ch_names \u001b[39m=\u001b[39m _check_stim_channel(stim_channel,\n\u001b[1;32m    392\u001b[0m                                                   edf_info[\u001b[39m'\u001b[39m\u001b[39mch_names\u001b[39m\u001b[39m'\u001b[39m])\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:373\u001b[0m, in \u001b[0;36m_read_header\u001b[0;34m(fname, exclude)\u001b[0m\n\u001b[1;32m    371\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m file detected\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m ext\u001b[39m.\u001b[39mupper())\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m ext \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mbdf\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39medf\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 373\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_edf_header(fname, exclude)\n\u001b[1;32m    374\u001b[0m \u001b[39melif\u001b[39;00m ext \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mgdf\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_gdf_header(fname, exclude), \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:519\u001b[0m, in \u001b[0;36m_read_edf_header\u001b[0;34m(fname, exclude)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read header information from EDF+ or BDF file.\"\"\"\u001b[39;00m\n\u001b[1;32m    517\u001b[0m edf_info \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mevents\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[0;32m--> 519\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     fid\u001b[39m.\u001b[39mread(\u001b[39m8\u001b[39m)  \u001b[39m# version (unused here)\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39m# patient ID\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/arooba/ssd/hira/eegformer_try_2/C:\\\\Users\\\\DELL\\\\Downloads\\\\tukl\\\\Implementations\\\\eegformer\\\\dataset_s\\\\abnormal\\\\eval\\\\0000036.edf'"
          ]
        }
      ],
      "source": [
        "import mne\n",
        "# Open the EDF file\n",
        "# edf_file = mne.io.read_raw_edf(r'/content/0000002.edf')\n",
        "edf_file = mne.io.read_raw_edf(r'C:\\Users\\DELL\\Downloads\\tukl\\Implementations\\eegformer\\dataset_s\\abnormal\\eval\\0000036.edf')\n",
        "# Print basic information about the file\n",
        "print(edf_file.info)\n",
        "# Plot the raw data\n",
        "edf_file.plot(duration=3000, n_channels=10)  # Adjust the duration and number of channels as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VWoJ9AK-UIh",
        "outputId": "dcb83641-f4d9-49bd-d2fd-deeb33e25e06"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'edf_file' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m original_sampling_freq \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m     17\u001b[0m target_sampling_freq \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eeg_data \u001b[39m=\u001b[39m edf_file\u001b[39m.\u001b[39mget_data()\n\u001b[1;32m     20\u001b[0m downsampled_eeg_data \u001b[39m=\u001b[39m downsample_signal(original_sampling_freq, target_sampling_freq, eeg_data)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Example usage: Print the dimensions of the downsampled EEG data\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'edf_file' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import signal\n",
        "\n",
        "def downsample_signal(original_sampling_freq, target_sampling_freq, signal_data):\n",
        "    original_num_samples = signal_data.shape[1]\n",
        "    original_time = np.arange(original_num_samples) / original_sampling_freq\n",
        "\n",
        "    target_num_samples = int(original_num_samples * (target_sampling_freq / original_sampling_freq))\n",
        "    target_time = np.arange(target_num_samples) / target_sampling_freq\n",
        "\n",
        "    downsampled_signal = signal.resample(signal_data, target_num_samples, axis=1)\n",
        "\n",
        "    return downsampled_signal\n",
        "\n",
        "# Example usage:\n",
        "original_sampling_freq = 200\n",
        "target_sampling_freq = 100\n",
        "eeg_data = edf_file.get_data()\n",
        "\n",
        "downsampled_eeg_data = downsample_signal(original_sampling_freq, target_sampling_freq, eeg_data)\n",
        "\n",
        "# Example usage: Print the dimensions of the downsampled EEG data\n",
        "print(\"original: \", eeg_data.shape)\n",
        "print(\"downsampled: \", downsampled_eeg_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pALLao7V-UIi",
        "outputId": "2a2775ab-99fb-444c-e892-9bb2189d838a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'downsampled_eeg_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     detrended_signal \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mdetrend(signal_data, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(detrended_signal, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m----> 7\u001b[0m detrended_eeg_data \u001b[39m=\u001b[39m detrend_signal(downsampled_eeg_data)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Example usage: Print the dimensions of the detrended EEG data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(detrended_eeg_data\u001b[39m.\u001b[39mshape)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'downsampled_eeg_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Detrend\n",
        "def detrend_signal(signal_data):\n",
        "    detrended_signal = signal.detrend(signal_data, axis=1)\n",
        "    return torch.tensor(detrended_signal, dtype=torch.float64)\n",
        "\n",
        "\n",
        "detrended_eeg_data = detrend_signal(downsampled_eeg_data)\n",
        "\n",
        "# Example usage: Print the dimensions of the detrended EEG data\n",
        "print(detrended_eeg_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\Downloads\\\\tukl\\\\Implementations\\\\eegformer\\\\dataset_s\\\\new.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m     label_reference \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(paths, labels))\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m label_reference\n\u001b[0;32m---> 18\u001b[0m label_reference \u001b[39m=\u001b[39m generate_label_ref(path)\n",
            "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mgenerate_label_ref\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m labels_file_path \u001b[39m=\u001b[39m path\n\u001b[1;32m      7\u001b[0m \u001b[39m# Load the CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(labels_file_path)\n\u001b[1;32m     10\u001b[0m paths \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mpaths\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     11\u001b[0m labels \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\Downloads\\\\tukl\\\\Implementations\\\\eegformer\\\\dataset_s\\\\new.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "path = r\"C:\\Users\\DELL\\Downloads\\tukl\\Implementations\\eegformer\\dataset_s\\new.csv\"\n",
        "def generate_label_ref(path):\n",
        "    '''generates label ref dictionary from\n",
        "    the csv file'''\n",
        "    labels_file_path = path\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(labels_file_path)\n",
        "\n",
        "    paths = df[\"paths\"].tolist()\n",
        "    labels = df[\"labels\"].tolist()\n",
        "\n",
        "    label_reference = dict(zip(paths, labels))\n",
        "    \n",
        "    return label_reference\n",
        "\n",
        "\n",
        "label_reference = generate_label_ref(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# class TrainDataset(Dataset):\n",
        "#     def __init__(self, fdeviceile_paths, label_ref transform=None):\n",
        "#         self.file_paths = file_paths\n",
        "#         self.transform = transform\n",
        "#         self.label_reference = label_ref\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.file_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         file_path = self.file_paths[idx]\n",
        "#         # Load and preprocess your data here\n",
        "#         edf_file = mne.io.read_raw_edf(file_path)\n",
        "#         eeg_data = edf_file.get_data()\n",
        "#         downsampled_eeg_data = self.downsample_signal(original_sampling_freq, target_sampling_freq, eeg_data)\n",
        "#         data = self.detrend_signal(downsampled_eeg_data)  # tensor\n",
        "#         label = label_reference[file_path]\n",
        "\n",
        "#         return data, label  # Return the preprocessed data and label\n",
        "    \n",
        "#     def downsample_signal(self, original_sampling_freq, target_sampling_freq, signal_data):\n",
        "#         original_num_samples = signal_data.shape[1]\n",
        "#         original_time = np.arange(original_num_samples) / original_sampling_freq\n",
        "\n",
        "#         target_num_samples = int(original_num_samples * (target_sampling_freq / original_sampling_freq))\n",
        "#         target_time = np.arange(target_num_samples) / target_sampling_freq\n",
        "\n",
        "#         downsampled_signal = signal.resample(signal_data, target_num_samples, axis=1)\n",
        "\n",
        "#         return downsampled_signal\n",
        "\n",
        "#     def detrend_signal(self, signal_data):\n",
        "#         detrended_signal = signal.detrend(signal_data, axis=1)\n",
        "#         return torch.tensor(detrended_signal, dtype=torch.float64)\n",
        "\n",
        "# # Example usage:\n",
        "# file_paths = ['path/to/file1', 'path/to/file2', 'path/to/file3']\n",
        "# dataset = CustomDataset(file_paths)\n",
        "\n",
        "# batch_size = 32\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Iterate over the dataset\n",
        "# for batch_data, batch_labels in dataloader:\n",
        "#     # Training or evaluation loop\n",
        "#     # ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainDataLoader:\n",
        "    def __init__(self, search_pattern, label_path):\n",
        "        self.file_paths = self.find_files_by_extension(search_pattern)\n",
        "        self.label_reference = self.generate_label_ref(label_path) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        # Load and preprocess your data here\n",
        "        edf_file = mne.io.read_raw_edf(file_path)\n",
        "        eeg_data = edf_file.get_data()\n",
        "        downsampled_eeg_data = self.downsample_signal(original_sampling_freq, target_sampling_freq, eeg_data)\n",
        "        data = self.detrend_signal(downsampled_eeg_data) * 1e6  # tensor\n",
        "        label = self.label_reference[file_path]\n",
        "\n",
        "        return data, label  # Return the preprocessed data and label    \n",
        "    \n",
        "    def generate_label_ref(self, path):\n",
        "        '''generates label ref dictionary from\n",
        "        the csv file'''\n",
        "        labels_file_path = path\n",
        "        # Load the CSV file into a DataFrame\n",
        "        df = pd.read_csv(labels_file_path)\n",
        "\n",
        "        paths = df[\"paths\"].tolist()\n",
        "        labels = df[\"labels\"].tolist()\n",
        "\n",
        "        label_reference = dict(zip(paths, labels))\n",
        "\n",
        "        return label_reference\n",
        "\n",
        "    def find_files_by_extension(self, search_pattern):\n",
        "        edf_files = glob.glob(search_pattern, recursive=True)\n",
        "        return edf_files  \n",
        "\n",
        "    def downsample_signal(self, original_sampling_freq, target_sampling_freq, signal_data):\n",
        "        original_num_samples = signal_data.shape[1]\n",
        "        original_time = np.arange(original_num_samples) / original_sampling_freq\n",
        "\n",
        "        target_num_samples = int(original_num_samples * (target_sampling_freq / original_sampling_freq))\n",
        "        target_time = np.arange(target_num_samples) / target_sampling_freq\n",
        "\n",
        "        downsampled_signal = signal.resample(signal_data, target_num_samples, axis=1)\n",
        "\n",
        "        return downsampled_signal\n",
        "\n",
        "    def detrend_signal(self, signal_data):\n",
        "        detrended_signal = signal.detrend(signal_data, axis=1)\n",
        "        return torch.tensor(detrended_signal, dtype=torch.float64)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TrainDataLoader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m TrainDataLoader(search_pattern, path)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TrainDataLoader' is not defined"
          ]
        }
      ],
      "source": [
        "dataset = TrainDataLoader(search_pattern, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m minn \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m minn:\n\u001b[1;32m      4\u001b[0m         minn \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "minn = 0\n",
        "for x, y in dataset:\n",
        "    if x.shape[1] < minn:\n",
        "        minn = x.shape[1]\n",
        "print(minn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ],
      "source": [
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(y)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_unsegmented_dataset(file_paths, label_reference):\n",
        "    xs = [] # list of tensors\n",
        "    ys = [] # list of numbers\n",
        "\n",
        "    xs_eval = []\n",
        "    ys_eval = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # do not add eval data to the dataset\n",
        "        if 'eval' in file_path:\n",
        "            # if the label for the filepath is present in the reference, only then append label and path to dataset\n",
        "            try:\n",
        "                ys_eval.append(label_reference[file_path]) # numbers\n",
        "            except:\n",
        "                continue\n",
        "            edf_file = mne.io.read_raw_edf(file_path)\n",
        "            eeg_data = edf_file.get_data()\n",
        "            downsampled_eeg_data = downsample_signal(original_sampling_freq, target_sampling_freq, eeg_data)\n",
        "            detrended_eeg_data = detrend_signal(downsampled_eeg_data)  # tensor\n",
        "            xs_eval.append(detrended_eeg_data)\n",
        "        else:\n",
        "            # if the label for the filepath is present in the reference, only then append label and path to dataset\n",
        "            try:\n",
        "                ys.append(label_reference[file_path]) # numbers\n",
        "            except:\n",
        "                continue\n",
        "            edf_file = mne.io.read_raw_edf(file_path)\n",
        "            eeg_data = edf_file.get_data()\n",
        "            downsampled_eeg_data = downsample_signal(original_sampling_freq, target_sampling_freq, eeg_data)\n",
        "            detrended_eeg_data = detrend_signal(downsampled_eeg_data)  # tensor\n",
        "            xs.append(detrended_eeg_data)\n",
        "    \n",
        "    return xs, ys, xs_eval, ys_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'label_reference' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, Y, X_eval, Y_eval \u001b[39m=\u001b[39m generate_unsegmented_dataset(file_paths, label_reference)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_reference' is not defined"
          ]
        }
      ],
      "source": [
        "X, Y, X_eval, Y_eval = generate_unsegmented_dataset(file_paths, label_reference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m         data_Y_eval \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data_Y_eval)\n\u001b[1;32m     40\u001b[0m         \u001b[39mreturn\u001b[39;00m data_X\u001b[39m.\u001b[39mdouble(), data_Y\u001b[39m.\u001b[39mdouble(), data_X_eval\u001b[39m.\u001b[39mdouble(), data_Y_eval\u001b[39m.\u001b[39mdouble()\n\u001b[0;32m---> 42\u001b[0m data_X, data_Y, data_X_eval, data_Y_eval \u001b[39m=\u001b[39m segmented_dataset_builder(X, Y, X_eval, Y_eval, \u001b[39m150\u001b[39m, \u001b[39m1\u001b[39m)            \n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "def segmented_dataset_builder(xs, ys, xs_eval, ys_eval, segment_length, dim):\n",
        "    data_X = []\n",
        "    data_Y = []\n",
        "\n",
        "    data_X_eval = []\n",
        "    data_Y_eval = []\n",
        "\n",
        "    index = 0\n",
        "    for tensor in xs:\n",
        "        label = ys[index]\n",
        "        for i in range(tensor.size(dim) // segment_length):\n",
        "            start_idx = i * segment_length\n",
        "            end_idx = start_idx + segment_length\n",
        "\n",
        "            # Extract the segment\n",
        "            segment = tensor[:, start_idx:end_idx]\n",
        "\n",
        "            # Update data arrays\n",
        "            data_X.append(segment.tolist())\n",
        "            data_Y.append(label)\n",
        "\n",
        "    for tensor in xs_eval:\n",
        "        label = ys_eval[index]\n",
        "        for i in range(tensor.size(dim) // segment_length):\n",
        "            start_idx = i * segment_length\n",
        "            end_idx = start_idx + segment_length\n",
        "\n",
        "            # Extract the segment\n",
        "            segment = tensor[:, start_idx:end_idx]\n",
        "\n",
        "            # Update data arrays\n",
        "            data_X_eval.append(segment.tolist())\n",
        "            data_Y_eval.append(label)\n",
        "\n",
        "        data_X = torch.tensor(data_X)\n",
        "        data_Y = torch.tensor(data_Y)\n",
        "        data_X_eval = torch.tensor(data_X_eval)\n",
        "        data_Y_eval = torch.tensor(data_Y_eval)\n",
        "        \n",
        "        return data_X.double(), data_Y.double(), data_X_eval.double(), data_Y_eval.double()\n",
        "    \n",
        "data_X, data_Y, data_X_eval, data_Y_eval = segmented_dataset_builder(X, Y, X_eval, Y_eval, 150, 1)            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(data_X\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(data_Y\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(data_X_eval\u001b[39m.\u001b[39mdtype)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_X' is not defined"
          ]
        }
      ],
      "source": [
        "print(data_X.shape)\n",
        "print(data_Y.shape)\n",
        "print(data_X_eval.dtype)\n",
        "print(data_Y_eval.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([label[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m ix])\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m x, y\n\u001b[0;32m---> 14\u001b[0m xb, yb \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "Cell \u001b[0;32mIn[37], line 3\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch\u001b[39m(split):\n\u001b[1;32m      2\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m         data \u001b[39m=\u001b[39m data_X\n\u001b[1;32m      4\u001b[0m         label \u001b[39m=\u001b[39m data_Y\n\u001b[1;32m      5\u001b[0m     \u001b[39melif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_X' is not defined"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = data_X\n",
        "        label = data_Y\n",
        "    elif split == \"eval\":\n",
        "        data = data_X_eval\n",
        "        label = data_Y_eval\n",
        "\n",
        "    ix = torch.randint(data.size(0), (B,))\n",
        "    x = torch.stack([data[i] for i in ix])\n",
        "    y = torch.stack([label[i] for i in ix])\n",
        "\n",
        "    return x, y\n",
        "xb, yb = get_batch('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xb' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(xb\u001b[39m.\u001b[39mshape, yb\u001b[39m.\u001b[39mshape)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xb' is not defined"
          ]
        }
      ],
      "source": [
        "print(xb.shape, yb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "position_embedding_table = nn.Embedding(L, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xb' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m xb \u001b[39m+\u001b[39m position_embedding_table(xb\u001b[39m.\u001b[39mlong())\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdouble()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(a\u001b[39m.\u001b[39mshape)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xb' is not defined"
          ]
        }
      ],
      "source": [
        "a = xb + position_embedding_table(xb.long()).squeeze().double()\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcmaS41U-UIj",
        "outputId": "98d0a4c9-6a3f-444a-c2b8-4b6d9eed3f01"
      },
      "outputs": [],
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, S=21, L=150, C=120):\n",
        "        super().__init__()\n",
        "        self.S = S  # no. of channels\n",
        "        self.L = L  # no. of sampled points\n",
        "        self.C = C  # depth of convolutional dimension\n",
        "        self.conv_layer_1 = nn.Conv1d(1, C, kernel_size=4)\n",
        "        self.conv_layer_2 = nn.Conv1d(C, C, kernel_size=4)\n",
        "        self.conv_layer_3 = nn.Conv1d(C, C, kernel_size=4)\n",
        "        self.conv_layer_4 = nn.Conv1d(C, C, kernel_size=4)\n",
        "        self.conv_layer_1.double()  # Update the data type of the convolutional layer weights to torch.float64\n",
        "        self.conv_layer_2.double()\n",
        "        self.conv_layer_3.double()\n",
        "        self.conv_layer_4.double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for i in range(self.S):\n",
        "            input_row = x[:, i:i+1, :]  # (batch_size=1, channels=1, length=num_columns)\n",
        "            output_tensor = self.conv_layer_1(input_row)\n",
        "            output_tensor = self.conv_layer_2(output_tensor)\n",
        "            output_tensor = self.conv_layer_3(output_tensor)\n",
        "            output_tensor = self.conv_layer_4(output_tensor)\n",
        "            outputs.append(output_tensor.unsqueeze(1))\n",
        "\n",
        "        output_tensor = torch.cat(outputs, dim=1)\n",
        "        return output_tensor\n",
        "\n",
        "# # Get the number of rows and columns\n",
        "# x = torch.tensor(detrended_eeg_data, dtype=torch.float64)\n",
        "# num_rows, num_columns = detrended_eeg_data.shape\n",
        "\n",
        "# # Define the Conv1D layer\n",
        "# conv1d_layer = CNN1D(S=21, L=L, C=C)\n",
        "# output = conv1d_layer(xb)\n",
        "\n",
        "# # Print output shape\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_tensor(tensor, dim, length):\n",
        "    tensor_shape = list(tensor.shape)\n",
        "    current_length = tensor_shape[dim]\n",
        "\n",
        "    if current_length >= length:\n",
        "        return tensor\n",
        "\n",
        "    padding_shape = tensor_shape.copy()\n",
        "    padding_shape[dim] = length - current_length\n",
        "\n",
        "    padding = torch.zeros(padding_shape, dtype=tensor.dtype, device=device)\n",
        "    padded_tensor = torch.cat((tensor, padding), dim=dim)\n",
        "\n",
        "    return padded_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "output = pad_tensor(torch.rand(3, 4).to('cuda'), 1, L)\n",
        "print(output.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kceSnJfD-UIo"
      },
      "outputs": [],
      "source": [
        "class RegionalHead(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, C, L):\n",
        "        # Parameters are head_size(d), no. of tokens(C), and input embedding size(L)\n",
        "        super().__init__()\n",
        "        self.block_size = C\n",
        "        self.n_embed = L\n",
        "        self.head_size = head_size \n",
        "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
        "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
        "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(self.block_size, self.block_size)))\n",
        "        self.dropout = nn.Dropout(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S_, C_, L_ = x.shape\n",
        "        x = x.view(S_, B, C_, L_)  # (B, T, C, L)\n",
        "        matrices = []\n",
        "\n",
        "        for spatial_mat in x:\n",
        "            inp = spatial_mat\n",
        "            # Below this, T is not the original T, but the head size\n",
        "            k = self.key(inp)   # (B, C, T)\n",
        "            q = self.query(inp) # (B, C, T)\n",
        "            # compute attention scores (\"affinities\")\n",
        "            wei = q @ k.transpose(-2,-1) * self.head_size**-0.5 # (B, C, T) @ (B, T, C) -> (B, C, C)\n",
        "            wei = F.softmax(wei, dim=-1) # (B, C, C)\n",
        "            wei = self.dropout(wei)\n",
        "            # perform the weighted aggregation of the values\n",
        "            v = self.value(inp) # (B, C, T)\n",
        "            out = wei @ v # (B, C, C) @ (B, C, T) -> (B, C, T)\n",
        "            matrices.append(out.tolist())\n",
        "\n",
        "        matrices = torch.tensor(matrices)\n",
        "        out = matrices.view(B, S_, C_, self.head_size)\n",
        "        return out.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYHuPgU_-UIp",
        "outputId": "043469fe-712a-49e1-f635-bc87b8fd3d4f"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'builtin_function_or_method' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rh \u001b[39m=\u001b[39m RegionalHead(head_size\u001b[39m=\u001b[39mL\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, C\u001b[39m=\u001b[39mC, L\u001b[39m=\u001b[39mL)\n\u001b[0;32m----> 2\u001b[0m rh_output \u001b[39m=\u001b[39m rh(torch\u001b[39m.\u001b[39;49mrand)  \u001b[39m# unsqueeze adds an extra batch dimension\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(rh_output\u001b[39m.\u001b[39mshape)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[44], line 17\u001b[0m, in \u001b[0;36mRegionalHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     B, S_, C_, L_ \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mshape\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(S_, B, C_, L_)  \u001b[39m# (B, T, C, L)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     matrices \u001b[39m=\u001b[39m []\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "rh = RegionalHead(head_size=L//5, C=C, L=L)\n",
        "rh_output = rh(torch.rand)  # unsqueeze adds an extra batch dimension\n",
        "print(rh_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "OjLdm8VI-UIr"
      },
      "outputs": [],
      "source": [
        "class RegionalMultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, C, L):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([RegionalHead(head_size, C, L) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(L, L).double()\n",
        "        self.dropout = nn.Dropout(0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1).to(device)\n",
        "        # out = self.dropout(self.proj(out)) # Instead of this line, we proceed as below:\n",
        "\n",
        "        # Implementing projection layer after the multihead attention module\n",
        "        b, s, c, l = out.shape\n",
        "        out = out.view(s, b, c, l)\n",
        "\n",
        "        matrices = []\n",
        "        for inp in out:\n",
        "            matrix = self.dropout(self.proj(inp))\n",
        "            matrices.append(matrix.tolist())\n",
        "\n",
        "        matrices = torch.tensor(matrices)\n",
        "        matrices = matrices.view(b, s, c, l).to('cuda')\n",
        "\n",
        "        return matrices.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9naGsKm-UIr",
        "outputId": "d6ab53fe-4c97-42f6-b01b-e51b6e1010e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mrh = RegionalMultiHeadAttention(5, head_size=L//5, C=C, L=L).to('cuda')\n",
        "mrh_output = mrh(torch.rand(B, S, C, L).double().to('cuda'))\n",
        "mrh_output.device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "a6xUXKnt-UIs"
      },
      "outputs": [],
      "source": [
        "class FeedFowardRegional(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, L): \n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(L, 4*L), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*L, L),\n",
        "            nn.Dropout(0.001),\n",
        "        ).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, s, c, l = x.shape\n",
        "        x = x.view(s, b, c, l)\n",
        "\n",
        "        matrices = []\n",
        "        for inp in x:\n",
        "            matrix = self.net(inp)\n",
        "            matrices.append(matrix.tolist())\n",
        "\n",
        "        matrices = torch.tensor(matrices).to(device)\n",
        "        # s_, b_, c_, l_ = matrices.shape\n",
        "        matrices = matrices.view(b, s, c, l)\n",
        "\n",
        "\n",
        "        return matrices.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCzKcU4n-UIs",
        "outputId": "0b698cc5-18e9-4bba-ac83-0ca36da21364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 21, 120, 150])\n"
          ]
        }
      ],
      "source": [
        "ffr = FeedFowardRegional(L).to('cuda')\n",
        "ffr_output = ffr(torch.rand(B, S, C, L).double().to('cuda'))\n",
        "print(ffr_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lOIGpr6x-UIt"
      },
      "outputs": [],
      "source": [
        "class BlockRegional(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, L, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        D = L // n_head\n",
        "        self.sa = RegionalMultiHeadAttention(n_head, D, C, L)\n",
        "        self.ffwd = FeedFowardRegional(L)\n",
        "        self.ln1 = nn.LayerNorm(L).double()\n",
        "        self.ln2 = nn.LayerNorm(L).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 21, 120, 150])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "br = BlockRegional(L, 5).to('cuda')\n",
        "br_output = br(torch.rand(B, S, C, L).double().to('cuda'))\n",
        "print(br_output.shape)\n",
        "print(br_output.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 120, 21, 150])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "def RegionalToSynchronousShapeShifter(tensor):\n",
        "    b, s, c, l = tensor.shape\n",
        "    return tensor.view(b, c, s, l)\n",
        "\n",
        "sync_input = RegionalToSynchronousShapeShifter(br_output)\n",
        "print(sync_input.shape)\n",
        "print(sync_input.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output of Regional block: [batch, S, C, L]\n",
        "\n",
        "\n",
        "Input expected by Synchronous Block: [batch, C, S, L]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SynchronousHead(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, S, L):\n",
        "        # Parameters are head_size(d), no. of tokens(C), and input embedding size(L)\n",
        "        super().__init__()\n",
        "        self.block_size = S\n",
        "        self.n_embed = L\n",
        "        self.head_size = head_size \n",
        "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
        "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
        "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False).double()\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(self.block_size, self.block_size)))\n",
        "        self.dropout = nn.Dropout(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, s, l= x.shape\n",
        "        x = x.view(c, b, s, l)  # (C, B, S, L)\n",
        "        matrices = []\n",
        "\n",
        "        for spatial_mat in x:\n",
        "            inp = spatial_mat\n",
        "            k = self.key(inp)   # (B, S, D)\n",
        "            q = self.query(inp) # (B, S, D)\n",
        "            # compute attention scores (\"affinities\")\n",
        "            wei = q @ k.transpose(-2,-1) * self.head_size**-0.5 # (B, S, D) @ (B, D, S) -> (B, S, S)\n",
        "            wei = F.softmax(wei, dim=-1) # (B, S, S)\n",
        "            wei = self.dropout(wei)\n",
        "            # perform the weighted aggregation of the values\n",
        "            v = self.value(inp) # (B, S, D)\n",
        "            out = wei @ v # (B, S, S) @ (B, S, D) -> (B, S, D)\n",
        "            matrices.append(out.tolist())\n",
        "\n",
        "        matrices = torch.tensor(matrices)\n",
        "        out = matrices.view(b, c, s, self.head_size)\n",
        "        return out.double().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 120, 21, 30])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "sh = SynchronousHead(head_size=L//5, S=S, L=L).to('cuda')\n",
        "sh_output = sh(sync_input) # unsqueeze adds an extra batch dimension\n",
        "print(sh_output.shape)\n",
        "print(sh_output.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SynchronousMultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, S, L):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SynchronousHead(head_size, S, L) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(L, L).double()\n",
        "        self.dropout = nn.Dropout(0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1).to('cuda')\n",
        "        # out = self.dropout(self.proj(out)) # Instead of this line, we proceed as below:\n",
        "\n",
        "        # Implementing projection layer after the multihead attention module\n",
        "        b, c, s, l = out.shape\n",
        "        out = out.view(c, b, s, l)\n",
        "\n",
        "        matrices = []\n",
        "        for inp in out:\n",
        "            matrix = self.dropout(self.proj(inp))  # inp is (B, S, L)\n",
        "            matrices.append(matrix.tolist())\n",
        "\n",
        "        matrices = torch.tensor(matrices)\n",
        "        matrices = matrices.view(b, c, s, l)\n",
        "\n",
        "        return matrices.double().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 120, 21, 150])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "mhs = SynchronousMultiHeadAttention(5, head_size=L//5, S=S, L=L).to('cuda')\n",
        "mhs_output = mhs(sync_input)\n",
        "print(mhs_output.shape)\n",
        "print(mhs_output.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedFowardSync(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, L): \n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(L, 4*L), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*L, L),\n",
        "            nn.Dropout(0.001),\n",
        "        ).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, s, l = x.shape\n",
        "        x = x.view(c, b, s, l)\n",
        "\n",
        "        matrices = []\n",
        "        for inp in x:\n",
        "            matrix = self.net(inp)\n",
        "            matrices.append(matrix.tolist())\n",
        "\n",
        "        matrices = torch.tensor(matrices)\n",
        "        # s_, b_, c_, l_ = matrices.shape\n",
        "        matrices = matrices.view(b, c, s, l)\n",
        "\n",
        "\n",
        "        return matrices.double().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 120, 21, 150])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "ffs = FeedFowardSync(L).to(device)\n",
        "ffs_output = ffs(mhs_output)\n",
        "print(ffs_output.shape)\n",
        "print(ffs_output.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BlockSync(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, L, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        D = L // n_head\n",
        "        self.sa = SynchronousMultiHeadAttention(n_head, D, S, L)\n",
        "        self.ffwd = FeedFowardSync(L)\n",
        "        self.ln1 = nn.LayerNorm(L).double()\n",
        "        self.ln2 = nn.LayerNorm(L).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 120, 21, 150])\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "bs = BlockSync(L, 5).to('cuda')\n",
        "bs_output = bs(sync_input)\n",
        "print(bs_output.shape)\n",
        "print(bs_output.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self, S, C, L, M):\n",
        "        super(TemporalTransformer, self).__init__()\n",
        "        self.C = C  # Number of channels\n",
        "        self.L = L  # Original temporal dimensionality\n",
        "        self.S = S  # Spatial dimension\n",
        "        self.M = M  # Compressed dimensionality\n",
        "        \n",
        "        self.patch_size = self.C * self.S  # Patch size\n",
        "        self.M_linear = nn.Linear(self.patch_size, self.patch_size).double()  # Learnable matrix M\n",
        "        \n",
        "    def forward(self, z5):\n",
        "        # z5: (B, C, S, D) input tensor\n",
        "        # Recuce the temporal dimension to M\n",
        "        z5_averaged = self.reduce_temporal_dimension(z5, self.M) # (B, C, S, M)\n",
        "        # Reshape the tensor to B, M, S*C\n",
        "        z5_reshaped = z5_averaged.reshape(z5.shape[0], -1, self.S*self.C)  # B, M, S*C\n",
        "        # Get latent vectors out of the current tensor\n",
        "        latent = self.M_linear(z5_reshaped) # (B, M, S*C)\n",
        "        return latent\n",
        "    \n",
        "    def reduce_temporal_dimension(self, input_tensor, M):\n",
        "        # input_tensor: (B, C, S, L) input tensor\n",
        "        # M: Compressed dimensionality\n",
        "\n",
        "        # Reshape the tensor to 3D\n",
        "        reshaped_tensor = input_tensor.view(-1, input_tensor.size(2), input_tensor.size(3))  # Shape: (B*C, S, L)\n",
        "\n",
        "        # Calculate the mean along the last dimension (L)\n",
        "        averaged_tensor = torch.mean(reshaped_tensor, dim=-1)  # Shape: (B*C, S)\n",
        "\n",
        "        # Resize the tensor to have the desired compressed dimensionality (M)\n",
        "        resized_tensor = torch.nn.functional.interpolate(averaged_tensor.unsqueeze(-1), size=M, mode='linear', align_corners=False)\n",
        "        resized_tensor = resized_tensor.squeeze(-1)\n",
        "\n",
        "        # Reshape back to 4D\n",
        "        output_tensor = resized_tensor.view(input_tensor.size(0), input_tensor.size(1), input_tensor.size(2), M)  # Shape: (B, C, S, M)\n",
        "\n",
        "        return output_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: torch.Size([3, 120, 21, 150])\n",
            "Output: torch.Size([3, 30, 2520])\n",
            "Output device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "temporal = TemporalTransformer(S, C, L, M=L//5).to(device)  # M is 15 here\n",
        "print(f'Input: {bs_output.shape}')\n",
        "temporal_without_attention_output = temporal(bs_output)\n",
        "print(f'Output: {temporal_without_attention_output.shape}')\n",
        "print(f'Output device: {temporal_without_attention_output.device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HeadTemporal(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, n_embed):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False).double()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False).double()\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False).double()\n",
        "\n",
        "        self.dropout = nn.Dropout(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "    \n",
        "class MultiHeadAttentionTemporal(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embed):\n",
        "        super().__init__()\n",
        "        self.n_embed = n_embed\n",
        "        self.heads = nn.ModuleList([HeadTemporal(head_size, self.n_embed) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(self.n_embed, self.n_embed).double()\n",
        "        self.dropout = nn.Dropout(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFowardTemporal(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(0.001),\n",
        "        ).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttentionTemporal(n_head, head_size, n_embd)\n",
        "        self.ffwd = FeedFowardTemporal(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd).double()\n",
        "        self.ln2 = nn.LayerNorm(n_embd).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def product_of_2_least_common_factors(num):\n",
        "    factors = []\n",
        "    \n",
        "    # Find all factors of the number\n",
        "    for i in range(1, num + 1):\n",
        "        if num % i == 0:\n",
        "            factors.append(i)\n",
        "        if len(factors) == 3:\n",
        "            break\n",
        "    \n",
        "    ans = 1\n",
        "    for factor in factors:\n",
        "        ans = ans * factor\n",
        "    \n",
        "    return ans\n",
        "\n",
        "product_of_2_least_common_factors(2520)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "# Till now the shape of tensor is: B, N, D\n",
        "# We don't want d to decrease, so we need to specify num_of_heads, and head_size carefully\n",
        "# The product_of_2_least_common_factors function allows us to achieve this.\n",
        "# It gives us a small number which can be used as the parameter: number of heads, as it's ouput divides d with no remainder\n",
        "no_of_heads_for_temoral_block = product_of_2_least_common_factors(S*C)\n",
        "print(no_of_heads_for_temoral_block)\n",
        "bt = TemporalBlock(S*C, no_of_heads_for_temoral_block).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([3, 30, 2520])\n",
            "Output shape: torch.Size([3, 30, 2520])\n",
            "Output shape: cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(f'Input shape: {temporal_without_attention_output.shape}')\n",
        "eeg_encoder_output = bt(temporal_without_attention_output)\n",
        "print(f'Output shape: {eeg_encoder_output.shape}')\n",
        "print(f'Output shape: {eeg_encoder_output.device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, B, M, S, C):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.B = B\n",
        "        self.M = M\n",
        "        self.S = S\n",
        "        self.C = C\n",
        "\n",
        "        # Define the layers\n",
        "        # Define the 1D convolutional filter - captures info along the convolutional dimension\n",
        "        self.l1_filter = nn.Conv1d(M*S, M*S, kernel_size=C).double()\n",
        "        # Define the l2 filter - captures info along spatial dimension\n",
        "        self.l2_filter = nn.Conv1d(M, M, kernel_size=S).double()\n",
        "        # PREDICTION NEURAL NETWORK\n",
        "        self.layer0 = nn.Linear(M, 256).double()\n",
        "        self.layer1 = nn.Linear(256, 64).double()\n",
        "        self.layer2 = nn.Linear(64, 1).double()\n",
        "        self.leaky_relu = nn.LeakyReLU().double()\n",
        "        self.sigmoid = nn.Sigmoid().double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder_to_decoder_shape_transition(x)\n",
        "\n",
        "        # Reshape from (B, M, S, C) to (B, M*S, C)\n",
        "        x = x.view(self.B, self.M*self.S, self.C)\n",
        "        # Apply the convolutional filter\n",
        "        x = self.l1_filter(x)  # reduces C dimension to 1\n",
        "        # Reshape the output tensor back to the desired shape (B, M, S)\n",
        "        x = x.view(self.B, self.M, self.S)\n",
        "        # apply\n",
        "        x = self.l2_filter(x)  # this filter reduces s dimension to 1\n",
        "        # Reshape\n",
        "        b, m, s =  x.shape \n",
        "        x = x.view(b, m*s)  # (B, M)\n",
        "    \n",
        "        # Pass the input through the layers with activations\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "    def encoder_to_decoder_shape_transition(self, matrix):\n",
        "        '''this function reshapes the oupput of encoder so that it is\n",
        "        suitable for the decoder'''\n",
        "        matrix = matrix.view(B, M, S, C)\n",
        "        return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5082],\n",
            "        [0.5078],\n",
            "        [0.5085]], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder(B, M, S, C).to(device)\n",
        "prediction = decoder(eeg_encoder_output)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EEGFormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EEGFormer(nn.Module):\n",
        "    def __init__(self, B, S, C, L, M):\n",
        "        super().__init__()\n",
        "        self.B = B\n",
        "        self.S = S\n",
        "        self.C = C\n",
        "        self.L = L\n",
        "        self.M = M\n",
        "        self.position_embedding_table = nn.Embedding(L, 1).to(device)\n",
        "        self.conv1d_layer = CNN1D(S=S, L=L, C=C).to(device)\n",
        "        self.br = BlockRegional(L, num_heads).to(device)\n",
        "        self.bs = BlockSync(L, num_heads).to(device)\n",
        "        self.temporal = TemporalTransformer(S, C, L, M=M) .to(device)\n",
        "        self.bt = TemporalBlock(S*C, n_head=product_of_2_least_common_factors(S*C)).to(device)  # nembd, nhead\n",
        "        self.decoder = Decoder(B, M, S, C).to(device)\n",
        "        self.position_embedding_table = nn.Embedding(L, 1).to(device)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # x is eeg segment\n",
        "        x = x + self.position_embedding_table(x.long()).squeeze().double()\n",
        "        x = self.conv1d_layer(x)\n",
        "        x = pad_tensor(x, dim=3, length=L)\n",
        "        x = self.br(x)\n",
        "        x = RegionalToSynchronousShapeShifter(x)\n",
        "        x = self.bs(x)\n",
        "        x = self.temporal(x)\n",
        "        x = self.bt(x)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, cols = x.shape\n",
        "            probabilities = x.view(B*cols,)\n",
        "            loss = F.binary_cross_entropy(probabilities, targets)   \n",
        "                     \n",
        "        return x, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_iters = 1\n",
        "eval_interval = 3\n",
        "log_file_path = \"loss_log.txt\"\n",
        "max_iters = 6\n",
        "learning_rate = 0.1\n",
        "# Create a list to store training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'eval']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            probs, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize your EEGFormer model\n",
        "device='cuda'\n",
        "model = EEGFormer(B, S, C, L, M)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "inp= torch.rand(B, S,L).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.5117],\n",
              "         [0.5123],\n",
              "         [0.5116]], device='cuda:0', dtype=torch.float64,\n",
              "        grad_fn=<SigmoidBackward0>),\n",
              " None)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: Train Loss 0.6812, Val Loss 0.6812\n",
            "Step 3: Train Loss 100.0000, Val Loss 100.0000\n",
            "Step 5: Train Loss 100.0000, Val Loss 100.0000\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    # Every once in a while, evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = losses['train']\n",
        "        val_loss = losses['eval']\n",
        "        print(f\"Step {iter}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
        "\n",
        "        # Append losses to the lists\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Log the losses to a text file\n",
        "        with open(log_file_path, \"a\") as log_file:\n",
        "            log_file.write(f\"Step {iter}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\\n\")\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAHACAYAAACoF1lmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWA0lEQVR4nO3dd3gUZcPF4bPphST0FIk06YTQQ8CCEglFpCnlRQUFsQCCymd5lSYqdqUJFgRRAUGlKE1EQYQkdKQ3IwQhoaZCCtn5/gis5gUkCUlms/nd17WX2ZnZ2bMPQ9zDszNrMQzDEAAAAAAgz5zMDgAAAAAAJQ1FCgAAAADyiSIFAAAAAPlEkQIAAACAfKJIAQAAAEA+UaQAAAAAIJ8oUgAAAACQTxQpAAAAAMgnF7MD2AOr1arjx4/Lx8dHFovF7DgAAAAATGIYhlJSUhQUFCQnp2vPO1GkJB0/flzBwcFmxwAAAABgJ+Li4lSlSpVrrqdISfLx8ZGUM1i+vr4mpwEAAABgluTkZAUHB9s6wrVQpCTbx/l8fX0pUgAAAACue8oPF5sAAAAAgHyiSAEAAABAPlGkAAAAACCfOEcqj7Kzs5WVlWV2DDgYZ2dnubi4cNl9AACAEoYilQepqak6duyYDMMwOwockJeXlwIDA+Xm5mZ2FAAAAOQRReo6srOzdezYMXl5ealSpUrMHKDQGIahzMxMnTp1SrGxsapVq9a/fukbAAAA7AdF6jqysrJkGIYqVaokT09Ps+PAwXh6esrV1VVHjhxRZmamPDw8zI4EAACAPOCfv/OImSgUFWahAAAASh7ewQEAAABAPlGkAAAAACCfTC1Sv/76q7p06aKgoCBZLBYtWrQo13rDMDR69GgFBgbK09NTEREROnjwYK5tzp49q379+snX11dly5bVwIEDlZqaWoyvovSoVq2aPvjgA7NjAAAAAKYztUilpaUpNDRUU6dOver6t956S5MmTdL06dMVExMjb29vRUZGKj093bZNv379tHv3bq1atUo//PCDfv31Vw0ePLi4XoJdslgs/3obO3Zsgfa7adOmGx7btm3basSIETe0DwAAAMBspl61r2PHjurYseNV1xmGoQ8++EAvv/yyunbtKkmaPXu2/P39tWjRIvXp00d79+7VihUrtGnTJjVv3lySNHnyZHXq1EnvvPOOgoKCiu212JMTJ07Yfv766681evRo7d+/37asTJkytp8Nw1B2drZcXK5/KFSqVKlwgwIAAAAllN1e/jw2Nlbx8fGKiIiwLfPz81NYWJiioqLUp08fRUVFqWzZsrYSJUkRERFycnJSTEyMunfvftV9Z2RkKCMjw3Y/OTk5z7kMw9CFrOwCvKIb5+nqnKerBwYEBNh+9vPzk8VisS1bs2aN7rzzTi1btkwvv/yydu7cqR9//FHBwcF65plnFB0drbS0NNWrV08TJkzINf7VqlXTiBEjbDNKFotFn3zyiZYuXaqVK1fqpptu0rvvvqt77723wK/x22+/1ejRo3Xo0CEFBgZq2LBhevbZZ23rP/zwQ73//vuKi4uTn5+fbrvtNn3zzTeSpG+++Ubjxo3ToUOH5OXlpSZNmmjx4sXy9vYucB4ApYM1O1sxs56X/1+rxDVaAcAcpxo8rJY9R5gdI8/stkjFx8dLkvz9/XMt9/f3t62Lj49X5cqVc613cXFR+fLlbdtczYQJEzRu3LgC5bqQla36o1cW6LE3as8rkfJyK5w/shdeeEHvvPOOatSooXLlyikuLk6dOnXSa6+9Jnd3d82ePVtdunTR/v37dfPNN19zP+PGjdNbb72lt99+W5MnT1a/fv105MgRlS9fPt+ZtmzZol69emns2LHq3bu3NmzYoCeffFIVKlTQgAEDtHnzZj311FP64osv1Lp1a509e1br1q2TlDML17dvX7311lvq3r27UlJStG7dOhmGUeAxAlA6WLOztXlqf4Wf/d7sKABQqsWnnjI7Qr7YbZEqSi+++KKeeeYZ2/3k5GQFBwebmKj4vfLKK7r77rtt98uXL6/Q0FDb/fHjx2vhwoVasmSJhg4des39DBgwQH379pUkvf7665o0aZI2btyoDh065DvTe++9p3bt2mnUqFGSpNq1a2vPnj16++23NWDAAB09elTe3t6655575OPjo6pVq6pJkyaScorUxYsX1aNHD1WtWlWSFBISku8MAEoXa3a2Nk95UC3PLVW2YdGmOiPlXYXfHQBghqo31zU7Qr7YbZG6/FG0hIQEBQYG2pYnJCSocePGtm1OnjyZ63EXL17U2bNnc3287X+5u7vL3d29QLk8XZ2155XIAj32Rnm6Ohfavv75cUhJSk1N1dixY7V06VJbKblw4YKOHj36r/tp1KiR7Wdvb2/5+vpe8WeSV3v37rWdD3dZmzZt9MEHHyg7O1t33323qlatqho1aqhDhw7q0KGDunfvLi8vL4WGhqpdu3YKCQlRZGSk2rdvr/vuu0/lypUrUBYAjs+ana3Nkx9Qy8RlyjYs2tb8TbXq8pjZsQAAJYTdfo9U9erVFRAQoNWrV9uWJScnKyYmRuHh4ZKk8PBwJSYmasuWLbZtfv75Z1mtVoWFhRVJLovFIi83F1NueTk/Kq/+97yhkSNHauHChXr99de1bt06bd++XSEhIcrMzPzX/bi6ul4xPlartdBy/pOPj4+2bt2quXPnKjAwUKNHj1ZoaKgSExPl7OysVatWafny5apfv74mT56sOnXqKDY2tkiyACjZsi9e1JbJ/f4uUS3eUnNKFAAgH0wtUqmpqdq+fbu2b98uKecCE9u3b9fRo0dlsVg0YsQIvfrqq1qyZIl27typhx56SEFBQerWrZskqV69eurQoYMeffRRbdy4UevXr9fQoUPVp0+fUnvFvoJav369BgwYoO7duyskJEQBAQH6888/izVDvXr1tH79+ity1a5dW87OObNxLi4uioiI0FtvvaXff/9df/75p37++WdJOSWuTZs2GjdunLZt2yY3NzctXLiwWF8DAPuXffGitk7upxaJy3XRcNL2lu+o+T2l+2szAAD5Z+pH+zZv3qw777zTdv/yeUv9+/fXrFmz9NxzzyktLU2DBw9WYmKibr31Vq1YsUIeHh62x3z11VcaOnSo2rVrJycnJ/Xs2VOTJk0q9tdS0tWqVUvfffedunTpIovFolGjRhXZzNKpU6ds5fmywMBAPfvss2rRooXGjx+v3r17KyoqSlOmTNGHH34oSfrhhx/0xx9/6Pbbb1e5cuW0bNkyWa1W1alTRzExMVq9erXat2+vypUrKyYmRqdOnVK9evWK5DUAKJlySlRftUj6URcNJ+0Ie0fNOg00OxYAoAQytUi1bdv2X6+qZrFY9Morr+iVV1655jbly5fXnDlziiJeqfLee+/pkUceUevWrVWxYkU9//zz+bosfH7MmTPnij+z8ePH6+WXX9b8+fM1evRojR8/XoGBgXrllVc0YMAASVLZsmX13XffaezYsUpPT1etWrU0d+5cNWjQQHv37tWvv/6qDz74QMnJyapatarefffda35PGYDSJ/viRW2b1EctklddKlHvqVmnh82OBQAooSwG14dWcnKy/Pz8lJSUJF9f31zr0tPTFRsbq+rVq+eaCQMKC8cYUPQuZmVq++S+ap78k7IMZ+0Mf09NOwwwOxYAwA79Wzf4J7u9ah8AAIXhYlamtk/qo+Ypq5VlOGtX6/fVNLK/2bEAACUcRQoA4LAuZmVqx6Teap7yc85MVOuJahr5oNmxAAAOgCIFAHBIF7MytWPi/WqWukaZhrN2t5mkpu0fMDsWAMBBUKQAAA4nKzNDOyfdr2apa5VpOGvPrVPU5O7/mB0LAOBAKFIAAIeSU6LuU9PUX3NK1G1T1Tiir9mxAAAOhiIFAHAYWZkZ2jmxp5qmrVOm4aI9t09V43Z9zI4FAHBAFCkAgEPIzEjX7kk91TTtN2UaLtp7xzQ1vquX2bEAAA6KIgUAKPEyM9K1e2IPNTm/XhmGq/a1nabQO+83OxYAwIE5mR0A9qtt27YaMWKE7X61atX0wQcf/OtjLBaLFi1adMPPXVj7AeD4ckpUd1uJ2t92OiUKAFDkKFIOqEuXLurQocNV161bt04Wi0W///57vve7adMmDR48+Ebj5TJ27Fg1btz4iuUnTpxQx44dC/W5/tesWbNUtmzZIn0OAEUrI/289kzspibnN+SUqDs/UqM77zM7FgCgFKBIOaCBAwdq1apVOnbs2BXrZs6cqebNm6tRo0b53m+lSpXk5eVVGBGvKyAgQO7u7sXyXABKpoz089o7sbsan49SuuGqA3d9rEZte5odCwBQSlCkHNA999yjSpUqadasWbmWp6amasGCBRo4cKDOnDmjvn376qabbpKXl5dCQkI0d+7cf93v/3607+DBg7r99tvl4eGh+vXra9WqVVc85vnnn1ft2rXl5eWlGjVqaNSoUcrKypKUMyM0btw47dixQxaLRRaLxZb5fz/at3PnTt11113y9PRUhQoVNHjwYKWmptrWDxgwQN26ddM777yjwMBAVahQQUOGDLE9V0EcPXpUXbt2VZkyZeTr66tevXopISHBtn7Hjh2688475ePjI19fXzVr1kybN2+WJB05ckRdunRRuXLl5O3trQYNGmjZsmUFzgIgt/QLado3sasaX4hWuuGqg+1mKOSOHmbHAgCUIlxsIr8MQ8o6b85zu3pJFst1N3NxcdFDDz2kWbNm6aWXXpLl0mMWLFig7Oxs9e3bV6mpqWrWrJmef/55+fr6aunSpXrwwQdVs2ZNtWzZ8rrPYbVa1aNHD/n7+ysmJkZJSUm5zqe6zMfHR7NmzVJQUJB27typRx99VD4+PnruuefUu3dv7dq1SytWrNBPP/0kSfLz87tiH2lpaYqMjFR4eLg2bdqkkydPatCgQRo6dGiusvjLL78oMDBQv/zyiw4dOqTevXurcePGevTRR6/7eq72+i6XqLVr1+rixYsaMmSIevfurTVr1kiS+vXrpyZNmmjatGlydnbW9u3b5erqKkkaMmSIMjMz9euvv8rb21t79uxRmTJl8p0DwJXSL6Rp/8SuCk3fpAuGmw5HfKqQ27qaHQsAUMpQpPIr67z0epA5z/3f45Kbd542feSRR/T2229r7dq1atu2raScj/X17NlTfn5+8vPz08iRI23bDxs2TCtXrtT8+fPzVKR++ukn7du3TytXrlRQUM54vP7661ec1/Tyyy/bfq5WrZpGjhypefPm6bnnnpOnp6fKlCkjFxcXBQQEXPO55syZo/T0dM2ePVve3jmvf8qUKerSpYvefPNN+fv7S5LKlSunKVOmyNnZWXXr1lXnzp21evXqAhWp1atXa+fOnYqNjVVwcLAkafbs2WrQoIE2bdqkFi1a6OjRo/q///s/1a1bV5JUq1Yt2+OPHj2qnj17KiQkRJJUo0aNfGcAcKX0C2k68M8SdfcMNbz1XrNjAQBKIT7a56Dq1q2r1q1b67PPPpMkHTp0SOvWrdPAgQMlSdnZ2Ro/frxCQkJUvnx5lSlTRitXrtTRo0fztP+9e/cqODjYVqIkKTw8/Irtvv76a7Vp00YBAQEqU6aMXn755Tw/xz+fKzQ01FaiJKlNmzayWq3av3+/bVmDBg3k7Oxsux8YGKiTJ0/m67n++ZzBwcG2EiVJ9evXV9myZbV3715J0jPPPKNBgwYpIiJCb7zxhg4fPmzb9qmnntKrr76qNm3aaMyYMQW6uAeA3NLPp+rAxC5qlL5J5w13/dF+FiUKAGAaZqTyy9UrZ2bIrOfOh4EDB2rYsGGaOnWqZs6cqZo1a+qOO+6QJL399tuaOHGiPvjgA4WEhMjb21sjRoxQZmZmocWNiopSv379NG7cOEVGRsrPz0/z5s3Tu+++W2jP8U+XP1Z3mcVikdVqLZLnknKuOPif//xHS5cu1fLlyzVmzBjNmzdP3bt316BBgxQZGamlS5fqxx9/1IQJE/Tuu+9q2LBhRZYHcGTp51N1cGIXNcrYqvOGu2IjZ6lB605mxwIAlGLMSOWXxZLz8Tozbnk4P+qfevXqJScnJ82ZM0ezZ8/WI488Yjtfav369erataseeOABhYaGqkaNGjpw4ECe912vXj3FxcXpxIkTtmXR0dG5ttmwYYOqVq2ql156Sc2bN1etWrV05MiRXNu4ubkpOzv7us+1Y8cOpaWl2ZatX79eTk5OqlOnTp4z58fl1xcXF2dbtmfPHiUmJqp+/fq2ZbVr19bTTz+tH3/8UT169NDMmTNt64KDg/X444/ru+++07PPPqtPPvmkSLICju5CWooOTrxHIZdK1J8dPqdEAQBMR5FyYGXKlFHv3r314osv6sSJExowYIBtXa1atbRq1Spt2LBBe/fu1WOPPZbrinTXExERodq1a6t///7asWOH1q1bp5deeinXNrVq1dLRo0c1b948HT58WJMmTdLChQtzbVOtWjXFxsZq+/btOn36tDIyMq54rn79+snDw0P9+/fXrl279Msvv2jYsGF68MEHbedHFVR2dra2b9+e67Z3715FREQoJCRE/fr109atW7Vx40Y99NBDuuOOO9S8eXNduHBBQ4cO1Zo1a3TkyBGtX79emzZtUr169SRJI0aM0MqVKxUbG6utW7fql19+sa0DkHcX0lJ0eNI9CsnYllOiOs5W/fCi/Y45AADygiLl4AYOHKhz584pMjIy1/lML7/8spo2barIyEi1bdtWAQEB6tatW5736+TkpIULF+rChQtq2bKlBg0apNdeey3XNvfee6+efvppDR06VI0bN9aGDRs0atSoXNv07NlTHTp00J133qlKlSpd9RLsXl5eWrlypc6ePasWLVrovvvuU7t27TRlypT8DcZVpKamqkmTJrluXbp0kcVi0eLFi1WuXDndfvvtioiIUI0aNfT1119LkpydnXXmzBk99NBDql27tnr16qWOHTtq3LhxknIK2pAhQ1SvXj116NBBtWvX1ocffnjDeYHS5HKJapixXWmGh450+kL1W139y8YBAChuFsMwDLNDmC05OVl+fn5KSkqSr69vrnXp6emKjY1V9erV5eHhYVJCODKOMeBK51OTFDupixpk7lCa4aG4Tl+oblh7s2MBAEqBf+sG/8TFJgAAdiWnRHVWg8ydSjU8dazzF6rb8m6zYwEAkAtFCgBgN9JSEnVk8j1/l6h7vlTdFhFmxwIA4AoUKQCAXUhLSdSRSZ1VP2uXUgxP/dXlK9Vt3s7sWAAAXBVFCgBgutTkc4qb3Fn1s3YrWV6K7zpXdZu2NTsWAADXRJECAJgqp0R1Ur2sPTkl6t55qt30DrNjAQDwryhSecTFDVFUOLZQmqUkndVfUzpfKlHeSug6T7Wb3G52LAAArovvkboOZ2dnSVJmZqbJSeCozp8/L0lydXU1OQlQvJITz+j45I6qm7VHSfJWQrevVYsSBQAoIZiRug4XFxd5eXnp1KlTcnV1lZMT3ROFwzAMnT9/XidPnlTZsmVtpR0oDZITz+jElI6qc3G/kuStU93nq1borWbHAgAgzyhS12GxWBQYGKjY2FgdOXLE7DhwQGXLllVAQIDZMYBik3TutBKmdlSdiweUqDI63X2+bgltY3YsAADyhSKVB25ubqpVqxYf70Ohc3V1ZSYKpcrlElX7Uok602OBbmnU2uxYAADkG0Uqj5ycnOTh4WF2DAAosZLOntLJDzuq9sWDOicfne35jWqGtDI7FgAABUKRAgAUuaSzp3RyagfVyj6kc/LVufu+Uc2GYWbHAgCgwChSAIAilXQmQac+7Kha2Yd1Tr5KvP8b1WhAiQIAlGwUKQBAkblcom7JPqyz8lVSr+9UvX4Ls2MBAHDDKFIAgCKReDpeZ6Z11C3Zf+iM/JTS+ztVr9fc7FgAABQKihQAoNCdO3VCZ6d3VM3sWJ1WWaX1/k7V6jUzOxYAAIWGIgUAKFTnTp3QuWkdVNP6Z06J6rNQVes2NTsWAACFiiIFACg0Z0/+paTpnVTjconqu1hV6zQ2OxYAAIWOIgUAKBRnEo4p+aNOqm49olMqpwv/WaSqtRubHQsAgCJBkQIA3LAzCceU8lFHVbcetZWomylRAAAHRpECANyQ0/FxSv24o6pZ43RS5ZXRb5FurhVqdiwAAIoURQoAUGCn448q7eNOf5eoB5Yo+JYQs2MBAFDkKFIAgAI5ffyI0j7tpKrWY0pQBWU9sETBtzQ0OxYAAMWCIgUAyLfTx4/o/KcdVdX6l+JVUdkPLVGVGg3MjgUAQLGhSAEA8uXU8T+V/klH3WwcV7wqKfuh73VTjXpmxwIAoFhRpAAAeXbyr1hlfNpJwZdKlLX/D7qpel2zYwEAUOwoUgCAPEk4dlhZMzor2DihE6oko/8PCqJEAQBKKYoUAOC6ckpUJ1Ux4nXcUlnq/4OCqtUxOxYAAKahSAEA/lV83CFd/KzzpRLlL8uA7xVYlRIFACjdKFIAgGuKP3pQ2TM7q4qRoOMWfzk9vFQBN9cyOxYAAKajSAEArurEkf0yZnXRTUaC/rL4y/mRZQoIvsXsWAAA2AWKFADgCsf/3C99fo+CjJM6ZgmQ68Bl8q9S0+xYAADYDYoUACCX43/ul2VWZwXqlI5ZAuU6cCklCgCA/0GRAgDYHI/dJ6fP71GATinOEiT3QctU+abqZscCAMDuUKQAAJKkv/7YK+fZXWwlyuPR5aoUVM3sWAAA2CUnswMAAMz31x+75Tw7ZybqqNNNlCgAAK6DGSkAKOWOHdol1y/vlb/O6IhTFXkPWqaKQVXNjgUAgF2jSAFAKXbs0C65fdlFlXVWR5yC5T14mSoG3Gx2LAAA7J5df7QvOztbo0aNUvXq1eXp6amaNWtq/PjxMgzDto1hGBo9erQCAwPl6empiIgIHTx40MTUAFAyxB3aaStRfzoFy3vwckoUAAB5ZNdF6s0339S0adM0ZcoU7d27V2+++abeeustTZ482bbNW2+9pUmTJmn69OmKiYmRt7e3IiMjlZ6ebmJyALBvcQd3yMNWom5WmcHLVTEg2OxYAACUGBbjn9M7duaee+6Rv7+/ZsyYYVvWs2dPeXp66ssvv5RhGAoKCtKzzz6rkSNHSpKSkpLk7++vWbNmqU+fPnl6nuTkZPn5+SkpKUm+vr5F8loAwF4cPbBdnnO6qZLOKdapqnwfW6YK/lXMjgUAgF3Iazew6xmp1q1ba/Xq1Tpw4IAkaceOHfrtt9/UsWNHSVJsbKzi4+MVERFhe4yfn5/CwsIUFRVlSmYAsGdH9m+X15yul0pUNfk9vpwSBQBAAdj1xSZeeOEFJScnq27dunJ2dlZ2drZee+019evXT5IUHx8vSfL398/1OH9/f9u6q8nIyFBGRobtfnJychGkBwD7cmTfVnnP666KStQfTtVU9vFlKl/5JrNjAQBQItn1jNT8+fP11Vdfac6cOdq6das+//xzvfPOO/r8889vaL8TJkyQn5+f7RYczHkBABzbkb1bbCXqsHN1lXtiBSUKAIAbYNdF6v/+7//0wgsvqE+fPgoJCdGDDz6op59+WhMmTJAkBQQESJISEhJyPS4hIcG27mpefPFFJSUl2W5xcXFF9yIAwGR/7t2sMl9fLlE1VOGJFSpXKdDsWAAAlGh2XaTOnz8vJ6fcEZ2dnWW1WiVJ1atXV0BAgFavXm1bn5ycrJiYGIWHh19zv+7u7vL19c11AwBHFLtnk3y/7q4KStIh55qq8MRyla147X9oAgAAeWPX50h16dJFr732mm6++WY1aNBA27Zt03vvvadHHnlEkmSxWDRixAi9+uqrqlWrlqpXr65Ro0YpKChI3bp1Mzc8AJgsdneM/Bbcp/JK1iHnmqr05HL5VfC//gMBAMB12XWRmjx5skaNGqUnn3xSJ0+eVFBQkB577DGNHj3ats1zzz2ntLQ0DR48WImJibr11lu1YsUKeXh4mJgcAMz1x64YlfvmPpVTsg4636LKTy6jRAEAUIjs+nukigvfIwXAkRzeGa3y396nckrRQZdaqvzkcvmVr2R2LAAASgSH+B4pAED+HP59g61EHXCprcpDVlCiAAAoAnb90T4AQN4d2rFeFRf2Ulml6oBLbfkPWS6/chXNjgUAgEOiSAGAAzi04zdVWthLfkrTfpc6Chy6XL5lK5gdCwAAh8VH+wCghDu4fd0/SlRdShQAAMWAGSkAKMEObvtV/ov7yFdp2udSTzcNWyYfv/JmxwIAwOFRpACghDqwda0ClvSRr85rn2t9VRm2TGV8y5kdCwCAUoGP9gFACXRg6xpbidpLiQIAoNhRpACghNm/+WcFLs4pUXtcGyqYEgUAQLHjo30AUILs27xaVb7vpzKWC9rjFqKqw36Qt09Zs2MBAFDqMCMFACXEvk0/2UrUbkoUAACmYkYKAEqAfTE/KnjZg/K2pGu3WyNVf+oHeZXxMzsWAAClFkUKAOzc3piVunnZQ5dKVKhqDF8qT28fs2MBAFCq8dE+ALBje6JXqOqlmahd7o0pUQAA2AmKFADYqT1Ry1Vt+UPysmRop3sT1XzqB0oUAAB2go/2AYAd2r1hmaqvHHCpRDVVreHfy8OrjNmxAADAJcxIAYCd2b1+qa1E/e7RjBIFAIAdokgBgB3Z9dsS1fjxcolqodqUKAAA7BJFCgDsxK51i1Vz1UB5WjK1w6OFag9fLA9Pb7NjAQCAq+AcKQCwAzt/XaxaqwfKw5KlHZ4tVeepRZQoAADsGDNSAGCynb8u/EeJClNdZqIAALB7FCkAMNHOtd+p9upH5WHJ0navcNUdvkjuHl5mxwIAANdBkQIAk/y+5lvV/nmw3C1Z2ubVWvWe+o4SBQBACcE5UgBggh2/LFDdNU/YSlSD4Qvl5u5hdiwAAJBHFCkAKGY7fp6vemufkJvlorZ536oGT31LiQIAoITho30AUIx2/DzPVqK2et+mhsO/o0QBAFACUaQAoJhs/2mu6q198lKJul0hw7+Vq5u72bEAAEABUKQAoBhsXzVH9dcNkZslW1vL3KGQ4d9QogAAKMEoUgBQxLb9+KXq/zZUbpZsbSnTViFPLaBEAQBQwnGxCQAoQltXfqGQDcPlasnWFp+7FPrU13JxdTM7FgAAuEHMSAFAEdm28nNbidrs044SBQCAA6FIAUAR2LpilhpueDqnRPlGqPFT8yhRAAA4ED7aBwCFbMuymQqNeUYuFqs2+96tJk/Nk7MLv24BAHAkzEgBQCHasmyGrURt8mtPiQIAwEFRpACgkGxZ+qlCY0ZeKlGRajpsLiUKAAAHRZECgEKw+YeP1XjjpRJVtqOaDptDiQIAwIHxf3kAuEGbv/9ITTY/L2eLoY1lO6n5sC/l5OxsdiwAAFCEmJECgBuwecn0v0tUuc6UKAAASgmKFAAU0KbFH6rJlhculah71HzoF5QoAABKCYoUABTApkVT1Wzrf+VsMRRT/l41HzqbEgUAQClCkQKAfNq0aIqabXtJThZDMRW6qsWQWZQoAABKGYoUAOTDxoWT1Gzby5dKVDe1eHImJQoAgFKIq/YBQB5t+m6imu8Yk1OiKvZQyydnyOLEv0cBAFAa8Q4AAPJg47fvq8Xvoy+VqJ6UKAAASjneBQDAdWz85j213DlWkhRd6X61fPJTShQAAKUcH+0DgH8Rs+Bdhe1+RZIUXbmXwh7/iBIFAAAoUgBwLTHz31bYnlclSdGVeyvs8emUKAAAIIkiBQBXFfP1mwrb+7okKdq/r8Ie+5ASBQAAbHhXAAD/I+brN/4uUQH9KFEAAOAKzEgBwD9Ez31drfa/mfNzQD+FDZ5CiQIAAFfg3QEAXBI99zVbiYoKfIgSBQAArokZKQCQFD1nvFodeEeSFBXUX60GfUCJAgAA10SRAlDqRX/1ilodfFeSFHXTALUa+D4lCgAA/CuKFIBSLfrLsWp16H1JUlSVR9TqkXcpUQAA4Lp4twCg1Ir+cszfJSp4ECUKAADkGTNSAEql6Nmj1OqPSZKkqOBHFT7wHZMTAQCAkoQiBaDUifr8JYXHTsn5+ebBCn/kbZMTAQCAkobPsAAoVaI+/+/fJarq45QoAABQIMxIASg1oma9oPA/p+X8XPVxhT/8psmJAABAScWMFIBSIWrm87YSFV1tCCUKAADcEGakADi8qM+eU/jRj3J+rj5U4f1fMzkRAAAo6ex+Ruqvv/7SAw88oAoVKsjT01MhISHavHmzbb1hGBo9erQCAwPl6empiIgIHTx40MTEAOxJ1IyRf5eoGk9RogAAQKGw6yJ17tw5tWnTRq6urlq+fLn27Nmjd999V+XKlbNt89Zbb2nSpEmaPn26YmJi5O3trcjISKWnp5uYHIDZDKtVUTOeVXjcJ5Kk6JrDFf7QeJNTAQAAR2ExDMMwO8S1vPDCC1q/fr3WrVt31fWGYSgoKEjPPvusRo4cKUlKSkqSv7+/Zs2apT59+uTpeZKTk+Xn56ekpCT5+voWWn4A5jCsVsV8NlKtjs2QJEXf8rRaPTDW3FAAAKBEyGs3sOsZqSVLlqh58+a6//77VblyZTVp0kSffPKJbX1sbKzi4+MVERFhW+bn56ewsDBFRUVdc78ZGRlKTk7OdQPgGAyrVdEznvm7RNV6hhIFAAAKnV0XqT/++EPTpk1TrVq1tHLlSj3xxBN66qmn9Pnnn0uS4uPjJUn+/v65Hufv729bdzUTJkyQn5+f7RYcHFx0LwJAsTGsVkV/OkLhf82UJEXXHqlW/caYnAoAADgiuy5SVqtVTZs21euvv64mTZpo8ODBevTRRzV9+vQb2u+LL76opKQk2y0uLq6QEgMwS06JGq7w4zn/0BJd+//U6j+jTE4FAAAclV0XqcDAQNWvXz/Xsnr16uno0aOSpICAAElSQkJCrm0SEhJs667G3d1dvr6+uW4ASi7DalX0J8MUfny2JCm6zvNq9Z+XTU4FAAAcmV0XqTZt2mj//v25lh04cEBVq1aVJFWvXl0BAQFavXq1bX1ycrJiYmIUHh5erFkBmMOwWhXz8VCFn/hSkhRT70W16vtfk1MBAABHZ9dfyPv000+rdevWev3119WrVy9t3LhRH3/8sT7++GNJksVi0YgRI/Tqq6+qVq1aql69ukaNGqWgoCB169bN3PAAipxhtSrmoyfVKmGuJCmm3n8V1vt5k1MBAIDSwK6LVIsWLbRw4UK9+OKLeuWVV1S9enV98MEH6tevn22b5557TmlpaRo8eLASExN16623asWKFfLw8DAxOYCillOinlCrhHmSpJj6Lyms13MmpwIAAKWFXX+PVHHhe6SAksWwWhUz/TG1OjlfkhTTYJTC7h9pcioAAOAI8toN7HpGCgD+l2G1KmbaYLU6tUCSFNNgtMLuf9bkVAAAoLShSAEoMQyrVRs/HKRWp7+VJG0MGauwnk+bnAoAAJRGFCkAJUJOiRqosNPfyWpYtCV0nFr2GG52LAAAUEpRpADYPWt2tjZ9+IjCziyS1bBoc+NX1LL7U2bHAgAApRhFCoBdu7JEjVfL7sPMjgUAAEo5ihQAu2XNztamqQMUdnZJzsf5mrymlt2GmB0LAACAIgXAPlmzs7V5an+Fnf0+p0Q1fV0tuj5pdiwAAABJFCkAdsiana3NUx5Uy3NLlW1YtK3ZBLW49wmzYwEAANhQpADYFWt2tjZPfkAtE5fllKjmb6p5l8fMjgUAAJALRQqA3ci+eFFbpzyglonLc0pUi7fU/J7BZscCAAC4AkUKgF3IvnhRWyf3U4ukFbpoOGlHy7fVvPMgs2MBAABcFUUKgOlySlRftUj6MadEhb2jZp0Gmh0LAADgmihSAEyVffGitk3qoxbJqy6VqPfUrNPDZscCAAD4VxQpAKa5mJWp7ZP7qnnyT8oynLUz/D016zDA7FgAAADXRZECYIqLWZnaPqmPmqesVpbhrF2t31fTyP5mxwIAAMgTihSAYncxK1M7JvVW85Sfc2aiWk9U08gHzY4FAACQZ04FeVBcXJyOHTtmu79x40aNGDFCH3/8caEFA+CYLmZlasfE+9Us5WdlGs7a1WYSJQoAAJQ4BSpS//nPf/TLL79IkuLj43X33Xdr48aNeumll/TKK68UakAAjiMrM0O/T7xPzVLXKNNw1p5bp6hJ+wfMjgUAAJBvBSpSu3btUsuWLSVJ8+fPV8OGDbVhwwZ99dVXmjVrVmHmA+AgsjIztHPSfWqaujanRN02VY3v/o/ZsQAAAAqkQOdIZWVlyd3dXZL0008/6d5775Uk1a1bVydOnCi8dAAcQlZmhnZO7KmmaeuUabhoz+1T1bhdH7NjAQAAFFiBZqQaNGig6dOna926dVq1apU6dOggSTp+/LgqVKhQqAEBlGyZGenaNbGHrUTtvWMaJQoAAJR4BSpSb775pj766CO1bdtWffv2VWhoqCRpyZIlto/8AUBmRrp2T+yhJmm/KcNw1d620xV6Vy+zYwEAANwwi2EYRkEemJ2dreTkZJUrV8627M8//5SXl5cqV65caAGLQ3Jysvz8/JSUlCRfX1+z4wAOIadEdVeT8xuUYbhqf9vpanTnfWbHAgAA+Fd57QYFmpG6cOGCMjIybCXqyJEj+uCDD7R///4SV6IAFL6M9PPaM7GbmpzfoHTDVQfu+pgSBQAAHEqBilTXrl01e/ZsSVJiYqLCwsL07rvvqlu3bpo2bVqhBgRQsmSkn9feid3V+HyU0g1XHWz3iULu6GF2LAAAgEJVoCK1detW3XbbbZKkb775Rv7+/jpy5Ihmz56tSZMmFWpAACVH+oU07ZvYVY0vRF8qUTMUcnt3s2MBAAAUugJd/vz8+fPy8fGRJP3444/q0aOHnJyc1KpVKx05cqRQAwIoGdIvpGn/xK4KTd+kC4abDkd8qpDbupodCwAAoEgUaEbqlltu0aJFixQXF6eVK1eqffv2kqSTJ09ysQagFEq/kKYD/yxRd89QQ0oUAABwYAUqUqNHj9bIkSNVrVo1tWzZUuHh4ZJyZqeaNGlSqAEB2Lf086k6MLGLGqVv0nnDXX+0n6WGt95rdiwAAIAiVeDLn8fHx+vEiRMKDQ2Vk1NOH9u4caN8fX1Vt27dQg1Z1Lj8OVAw6edTdXBiF4VkbNV5w12xkbPUoHUns2MBAAAUWF67QYHOkZKkgIAABQQE6NixY5KkKlWq8GW8QClyIS1FhyZ1UUjGNp033PVnh8/VILyj2bEAAACKRYE+2me1WvXKK6/Iz89PVatWVdWqVVW2bFmNHz9eVqu1sDMCsDMX0lJ0eNI9f5eojrNVnxIFAABKkQLNSL300kuaMWOG3njjDbVp00aS9Ntvv2ns2LFKT0/Xa6+9VqghAdiPyyWqYcZ2pRkeOtpptuqHRZodCwAAoFgV6BypoKAgTZ8+Xffem/uE8sWLF+vJJ5/UX3/9VWgBiwPnSAF5cz41SbGTuqhB5g6lGR6K6/SF6oa1NzsWAABAoSnSc6TOnj171QtK1K1bV2fPni3ILgHYuZwS1VkNMncq1fDUsc5fqG7Lu82OBQAAYIoCnSMVGhqqKVOmXLF8ypQpatSo0Q2HAmBf0lIS9eelEpVieOpYl68oUQAAoFQr0IzUW2+9pc6dO+unn36yfYdUVFSU4uLitGzZskINCMBcaSmJOjKps+pn7VKK4anjXeaobvO7zI4FAABgqgLNSN1xxx06cOCAunfvrsTERCUmJqpHjx7avXu3vvjii8LOCMAkqcnndHRSJ9XP2qVkeelE13mqQ4kCAAAo+BfyXs2OHTvUtGlTZWdnF9YuiwUXmwCulJp8TnGTO6le1h4ly0vx985T7aZ3mB0LAACgSBX5F/ICcFwpSWf115TOl0qUtxK6zlPtJrebHQsAAMBuUKQA5JKceEYnpnRS3Yv7lCRvnez2tWo1vs3sWAAAAHaFIgXAJqdEdVSdi/uVJG+d6j5ftUJvNTsWAACA3clXkerRo8e/rk9MTLyRLABMlHTutBKmdlSdiweUqDI63X2+bgltY3YsAAAAu5SvIuXn53fd9Q899NANBQJQ/C6XqNqXStSZHgt0S6PWZscCAACwW/kqUjNnziyqHABMknT2lE5+2FG1Lx7UOfnobM9vVDOkldmxAAAA7BrnSAGlWNLZUzo5tYNqZR/SOfnq3H3fqGbDMLNjAQAA2D2KFFBKJZ1J0KkPO6pW9mGdk68S7/9GNRpQogAAAPKCIgWUQpdL1C3Zh3VWvkrq9Z2q129hdiwAAIASgyIFlDKJp+N1ZlpH3ZL9h87ITym9v1P1es3NjgUAAFCiUKSAUuTcqRM6O72jambH6rTKKq33d6pWr5nZsQAAAEocihRQSpw7dULnpnVQTeufOSWqz0JVrdvU7FgAAAAlEkUKKAXOnvxLSdM7qcblEtV3sarWaWx2LAAAgBKLIgU4uDMJx5T8USdVtx7RKZXThf8sUtXajc2OBQAAUKJRpAAHdibhmFI+6qjq1qO2EnUzJQoAAOCGUaQAB3U6Pk6pH3dUNWucTqq8Mvot0s21Qs2OBQAA4BAoUoADOh1/VGkfd/q7RD2wRMG3hJgdCwAAwGFQpAAHc/r4EaV92klVrceUoArKemCJgm9paHYsAAAAh0KRAhzI6eNHdP7Tjqpq/Uvxqqjsh5aoSo0GZscCAABwOBQpwEGcOv6n0j/pqJuN44pXJWU/9L1uqlHP7FgAAAAOycnsAPnxxhtvyGKxaMSIEbZl6enpGjJkiCpUqKAyZcqoZ8+eSkhIMC8kYIKTf8Uq/ZOOCr5Uoqz9f6BEAQAAFKESU6Q2bdqkjz76SI0aNcq1/Omnn9b333+vBQsWaO3atTp+/Lh69OhhUkqg+CUcO6zMT3NK1IlLJSqoel2zYwEAADi0ElGkUlNT1a9fP33yyScqV66cbXlSUpJmzJih9957T3fddZeaNWummTNnasOGDYqOjjYxMVA8Eo4dVtaMTqpinNBxS2UZA5ZSogAAAIpBiShSQ4YMUefOnRUREZFr+ZYtW5SVlZVred26dXXzzTcrKiqquGMCxSo+7tClEhWv4xZ/WQb8oKBqdcyOBQAAUCrY/cUm5s2bp61bt2rTpk1XrIuPj5ebm5vKli2ba7m/v7/i4+Ovuc+MjAxlZGTY7icnJxdaXqA4xB89qOyZnVXFSNBxi7+cHl6qgJtrmR0LAACg1LDrGam4uDgNHz5cX331lTw8PAptvxMmTJCfn5/tFhwcXGj7BoraiSP7ZZ3ZWTcZCfrL4i+nR5ZRogAAAIqZXRepLVu26OTJk2ratKlcXFzk4uKitWvXatKkSXJxcZG/v78yMzOVmJiY63EJCQkKCAi45n5ffPFFJSUl2W5xcXFF/EqAwnH8z/0yZt2jICNBxywBchm4XAHBt5gdCwAAoNSx64/2tWvXTjt37sy17OGHH1bdunX1/PPPKzg4WK6urlq9erV69uwpSdq/f7+OHj2q8PDwa+7X3d1d7u7uRZodKGzH/9wvy6zOCtQpHbMEynXgUvlXqWl2LAAAgFLJrouUj4+PGjZsmGuZt7e3KlSoYFs+cOBAPfPMMypfvrx8fX01bNgwhYeHq1WrVmZEBorE8dh9cvr8HgXolOIsQXIftEyVb6pudiwAAIBSy66LVF68//77cnJyUs+ePZWRkaHIyEh9+OGHZscCCs1ff+yV8+wuthLl8ehyVQqqZnYsAACAUs1iGIZhdgizJScny8/PT0lJSfL19TU7DmDz1x+75Tz7XgXotI463STPQcsoUQAAAEUor92gxM9IAY7q2KFdcv3yXvnrjI44VZH3oGWqGFTV7FgAAAAQRQqwS8cO7ZLbl11UWWd1xClY3oOXqWLAzWbHAgAAwCV2fflzoDSKO7TTVqL+dAqW9+DllCgAAAA7Q5EC7EjcwR3ysJWom1Vm8HJVDOALowEAAOwNRQqwE0cPbJfHV11VSecU61RVPo9RogAAAOwV50gBduDI/u3ynttVFZWoWKdq8nt8mcpXvsnsWAAAALgGZqQAkx3Zt9VWov6gRAEAAJQIzEgBJjqyd4u8v+6hikrUYefqKv/4cpWrFGh2LAAAAFwHRQowyZ97N8vn6x6qoCQddq6hCk8sV9mKAWbHAgAAQB7w0T7ABLF7Nsn36+6qoCQdcq5JiQIAAChhmJECilns7hj5LbhP5ZWsQ841VenJ5fKr4G92LAAAAOQDM1JAMfpjV4zKXipRB51voUQBAACUUMxIAcXk8M5olf/2PpVTig661FLlJ5fLr3wls2MBAACgAJiRAorB4d832ErUAZfaqjxkBSUKAACgBGNGCihih3asV8WFvVRWqTrgUlv+Q5bLr1xFs2MBAADgBlCkgCJ0aMdvqrSwl/yUpv0udRQ4dLl8y1YwOxYAAABuEEUKKCIHt69T5UW9L5Wougoatlw+fuXNjgUAAIBCwDlSQBE4uO1X+V8qUftc6lGiAAAAHAwzUkAhO7B1rQKW9JGvzmuva30FD1umMr7lzI4FAACAQsSMFFCIDmxd848S1YASBQAA4KAoUkAh2b/5ZwUuzilRe1wbKnjYUkoUAACAg+KjfUAh2Ld5tap8309lLBe0xy1EVYf9IG+fsmbHAgAAQBGhSAE3aN+mn1TlhwdUxnJBu91CVP2ppfIq42d2LAAAABQhihRwA/bF/KjgZQ/K25Ku3W6NVP2pHyhRAAAApQDnSAEFtDdmpa1E7XJvrBrDl1GiAAAASglmpIAC2BO9QtWWPyQvS4Z2uTdWzad+kKe3j9mxAAAAUEyYkQLyaU/UcluJ2unehBIFAABQCjEjBeTD7g3LVH3lgEslqqlqDf9eHl5lzI4FAACAYsaMFJBHu9cvtZWo3z2aU6IAAABKMYoUkAe7fluiGj9eLlEtVHv4EkoUAABAKUaRAq5j17rFqrlqoDwtmdrh0UK1hy+Wh6e32bEAAABgIs6RAv7Fzl8Xq9bqgfKwZGmHZ0vVHb5Y7h5eZscCAACAyZiRAq5h568L/1GiwihRAAAAsKFIAVexc+13qr36UXlYsrTdK1x1hy+iRAEAAMCGj/YB/+P3Nd+qzi+Pyd2SpW1erdVg+EK5uXuYHQsAAAB2hBkp4B92/LKAEgUAAIDrYkYKuGTHz/NVb+0TcrNc1DbvW9XgqW8pUQAAALgqZqQASTt+nmcrUVu9b1PD4d9RogAAAHBNFCmUett/mqt6a5+8VKJuV8jwb+Xq5m52LAAAANgxihRKte2r5qj+uiFys2Rra5k7FDL8G0oUAAAArosihVJr249fqv5vQ+VmydaWMm3ViBIFAACAPKJIoVTauvILNVz/VE6J8rlLocMXyMXVzexYAAAAKCEoUih1tq38XCEbhsvVkq3NPu0U+tTXlCgAAADkC0UKpcrWFbPUcMPTOSXKN0KNn5pHiQIAAEC+8T1SKDW2LJup0Jhn5GKxarPv3Wry1Dw5u/BXAAAAAPnHjBRKhS3LZthK1Ca/9pQoAAAA3BCKFBzelqWfKjRm5KUS1UFNh82lRAEAAOCGUKTg0Db/8LEab7xUosp2VNNhX1GiAAAAcMN4RwmHtfn7j9Rk8/NythjaWLaTmg/7Uk7OzmbHAgAAgANgRgoOafOS6X+XqHKdKVEAAAAoVBQpOJzNS6apyZYXLpWoe9R86BeUKAAAABQqihQcyqZFU9V0y4tythiKKX+vmg+dTYkCAABAoaNIwWFsWjRFzba9JCeLoZgK3dRiyCxKFAAAAIoERQoOYePCSWq27eW/S9STn1GiAAAAUGS4ah9KvE3fTVTzHWNySlTFHmr55AxZnPg3AgAAABQd3m2iRNv47ftq8fvoSyWqJyUKAAAAxYJ3nCixNn7znlruHCtJiq50v1o++SklCgAAAMWCj/ahRIpZ8K7Cdr8iSYqu3Ethj39EiQIAAECxoUihxImZ/7bC9rwqSYr276Owx6ZRogAAAFCsKFIoUWK+flNhe1+XJEX791XYYx9SogAAAFDs7Pod6IQJE9SiRQv5+PiocuXK6tatm/bv359rm/T0dA0ZMkQVKlRQmTJl1LNnTyUkJJiUGEUp5us3/i5RAf0oUQAAADCNXb8LXbt2rYYMGaLo6GitWrVKWVlZat++vdLS0mzbPP300/r++++1YMECrV27VsePH1ePHj1MTI2iED33dYXtnSBJigp8QGGDp1CiAAAAYBqLYRiG2SHy6tSpU6pcubLWrl2r22+/XUlJSapUqZLmzJmj++67T5K0b98+1atXT1FRUWrVqlWe9pucnCw/Pz8lJSXJ19e3KF8CCiB67mtqtf8tSVJU4ENq9ehEShQAAACKRF67QYl6N5qUlCRJKl++vCRpy5YtysrKUkREhG2bunXr6uabb1ZUVNQ195ORkaHk5ORcN9in6Dnj/y5RQf0pUQAAALALJeYdqdVq1YgRI9SmTRs1bNhQkhQfHy83NzeVLVs217b+/v6Kj4+/5r4mTJggPz8/2y04OLgoo6OAor96Ra0OvCNJirrpYbUa9AElCgAAAHahxLwrHTJkiHbt2qV58+bd8L5efPFFJSUl2W5xcXGFkBCFKfrLsWp18F1JUlSVR9Rq4HuUKAAAANiNEnH586FDh+qHH37Qr7/+qipVqtiWBwQEKDMzU4mJiblmpRISEhQQEHDN/bm7u8vd3b0oI+MGRH85Rq0OfSBJigoepFYPv02JAgAAgF2x63enhmFo6NChWrhwoX7++WdVr1491/pmzZrJ1dVVq1evti3bv3+/jh49qvDw8OKOi0IQPXvUP0rUowof+C4lCgAAAHbHrmekhgwZojlz5mjx4sXy8fGxnffk5+cnT09P+fn5aeDAgXrmmWdUvnx5+fr6atiwYQoPD8/zFftgP6I+f0nhsVNyfr55sMIfedvkRAAAAMDV2fXlzy0Wy1WXz5w5UwMGDJCU84W8zz77rObOnauMjAxFRkbqww8//NeP9v0vLn9uvqjP/6vw2Kk5P1d9XOEPv2lyIgAAAJRGee0Gdl2kigtFylxRs15Q+J/Tcn6u9oTCB7xhciIAAACUVg75PVJwPFEzn7eVqOhqQyhRAAAAKBHs+hwpOLaoz55T+NGPcn6uPlTh/V8zOREAAACQNxQpmCJqxkiFx30iSYqu8ZTCHxpvciIAAAAg7yhSKFaG1aromf+n8LhPJUnRNYer1YOvmJwKAAAAyB+KFIqNYbUq5rORCj82Q5IUfcvTavXAWHNDAQAAAAVAkUKxMKxWRc94RuF/zZQkRdd6Vq36jTY5FQAAAFAwFCkUOcNqVfSnIxR+/HNJUnTtkWr1n1EmpwIAAAAKjiKFIpVTooYr/PhsSVJ0nefUqu9LJqcCAAAAbgxFCkXGsFoV/ckwhZ/4UpIUXed5ter7X5NTAQAAADeOIoUiYVitivl4qMLjv5IkxdR7Ua16v2ByKgAAAKBwUKRQ6AyrVTEfPalWCXMlSTH1/quw3s+bnAoAAAAoPBQpFKqcEvWEWiXMkyTF1H9JYb2eMzkVAAAAULgoUig0htWqmOmPqdXJ+ZKkmAajFHb/SJNTAQAAAIWPIoVCYVitipk2WK1OLZAkxTQYrbD7nzU5FQAAAFA0KFK4YYbVqo0fDlKr099KkjaGjFVYz6dNTgUAAAAUHYoUbkhOiRqosNPfyWpYtCV0nFr2GG52LAAAAKBIUaRQYNbsbG368BGFnVkkq2HR5savqGX3p8yOBQAAABQ5ihQK5MoSNV4tuw8zOxYAAABQLChSyDdrdrY2TR2gsLNLcj7O1+Q1tew2xOxYAAAAQLGhSCFfrNnZ2jy1v8LOfp9Topq+rhZdnzQ7FgAAAFCsKFLIM2t2tjZPeVAtzy1VtmHRtmYT1OLeJ8yOBQAAABQ7ihTyxJqdrc2TH1DLxGU5Jar5m2re5TGzYwEAAACmoEjhurIvXtTWKQ+oZeLynBLV4i01v2ew2bEAAAAA01Ck8K+yL17U1sn91CJphS4aTtrR8m017zzI7FgAAACAqShSuKacEtVXLZJ+zClRYe+oWaeBZscCAAAATEeRwlVlX7yobZP6qEXyqksl6j016/Sw2bEAAAAAu0CRwhUuZmVq++S+ap78k7IMZ+0Mf0/NOgwwOxYAAABgNyhSyOViVqa2T+qj5imrlWU4a1fr99U0sr/ZsQAAAAC7QpGCzcWsTO2Y1FvNU37OmYlqPVFNIx80OxYAAABgdyhSkHSpRE28X81S1yjTcNbuNpPUtP0DZscCAAAA7BJFCsrKzNDOSferWepaZRrO2nPrFDW5+z9mxwIAAADsFkWqlMspUfepaeqvOSXqtqlqHNHX7FgAAACAXaNIlWJZmRnaObGnmqatU6bhoj23T1Xjdn3MjgUAAADYPYpUKZWZka7dk3qqadpvyjRctPeOaWp8Vy+zYwEAAAAlAkWqFMrMSNfuiT3U5Px6ZRiu2td2mkLvvN/sWAAAAECJQZEqZXJKVHc1Ob9BGYar9redrtA77zM7FgAAAFCiUKRKkYz089o7qYeanI9SuuGqg3d9rEZ39DA7FgAAAFDiUKRKiYz089o7sbsaX4jOKVHtPlHI7d3NjgUAAACUSBSpUiD9Qpr2T+qmxhc2XipRMxRye1ezYwEAAAAlFkXKwaVfSNP+iV0Vmr5JFww3HY74VCG3UaIAAACAG0GRcmDpF9J04J8l6u4ZanjrvWbHAgAAAEo8ipSDSj+fqgOT7lWj9C06b7grtv1MNWzT2exYAAAAgEOgSDmg9POpOjixixplbM0pUZGz1KB1J7NjAQAAAA6DIuVgLqSl6NCkLgrJ2Kbzhrv+7PC5GoR3NDsWAAAA4FAoUg7kQlqKDk+6RyEZ23NKVMfZqt+qg9mxAAAAAIdDkXIQl0tUw4ztSjM8dLTTbNUPizQ7FgAAAOCQKFIO4HxqkmIndVHDzB1KMzwU1+kL1Qtrb3YsAAAAwGFRpEq4nBLVWQ0ydyrV8NSxzl+obsu7zY4FAAAAODSKVAmWlpKoI5PvUYPMnUoxPPVXl69Ut3k7s2MBAAAADo8iVUKlpSTqyKTOqp+1SymGp453maO6ze8yOxYAAABQKlCkSqDU5HOKm9xZ9bN2K1leiu86V3WatjU7FgAAAFBqUKRKmJwS1Un1svbklKh756l20zvMjgUAAACUKhSpEiQl6az+mtL5UonyVkLXeard5HazYwEAAAClDkWqhEhOPKMTUzqp7sV9SpK3Tnb7WrUa32Z2LAAAAKBUokiVADklqqPqXNyvJHnrVPf5qhV6q9mxAAAAgFKLImXnks6dVsLUjqpz8YASVUanu8/XLaFtzI4FAAAAlGoUKTt2uUTVvlSizvRYoFsatTY7FgAAAFDqUaTsVNLZUzr5YUfVvnhQ5+Sjsz2/Uc2QVmbHAgAAACCKlF1KOntKJ6d2UK3sQzonX5277xvVbBhmdiwAAAAAlziZHaCwTJ06VdWqVZOHh4fCwsK0ceNGsyMVSNKZBJ2aGmkrUYn3f6MalCgAAADArjhEkfr666/1zDPPaMyYMdq6datCQ0MVGRmpkydPmh0tX5LOJOjUhx11S/ZhnZWvEnt9p+oNKFEAAACAvXGIIvXee+/p0Ucf1cMPP6z69etr+vTp8vLy0meffWZ2tHz58/dfVe1irM7IT8m9F6p6/RZmRwIAAABwFSX+HKnMzExt2bJFL774om2Zk5OTIiIiFBUVZWKy/Au9835tzUhThaohqlavmdlxAAAAAFxDiS9Sp0+fVnZ2tvz9/XMt9/f31759+676mIyMDGVkZNjuJycnF2nG/GjaYYDZEQAAAABch0N8tC+/JkyYID8/P9stODjY7EgAAAAASpASX6QqVqwoZ2dnJSQk5FqekJCggICAqz7mxRdfVFJSku0WFxdXHFEBAAAAOIgSX6Tc3NzUrFkzrV692rbMarVq9erVCg8Pv+pj3N3d5evrm+sGAAAAAHlV4s+RkqRnnnlG/fv3V/PmzdWyZUt98MEHSktL08MPP2x2NAAAAAAOyCGKVO/evXXq1CmNHj1a8fHxaty4sVasWHHFBSgAAAAAoDBYDMMwzA5htuTkZPn5+SkpKYmP+QEAAAClWF67QYk/RwoAAAAAihtFCgAAAADyiSIFAAAAAPlEkQIAAACAfKJIAQAAAEA+UaQAAAAAIJ8oUgAAAACQTxQpAAAAAMgnF7MD2IPL30mcnJxschIAAAAAZrrcCS53hGuhSElKSUmRJAUHB5ucBAAAAIA9SElJkZ+f3zXXW4zrVa1SwGq16vjx4/Lx8ZHFYjE1S3JysoKDgxUXFydfX19TszgixrdoMb5Fi/EtWoxv0WJ8ixbjW7QY36Jlb+NrGIZSUlIUFBQkJ6drnwnFjJQkJycnValSxewYufj6+trFgeSoGN+ixfgWLca3aDG+RYvxLVqMb9FifIuWPY3vv81EXcbFJgAAAAAgnyhSAAAAAJBPFCk74+7urjFjxsjd3d3sKA6J8S1ajG/RYnyLFuNbtBjfosX4Fi3Gt2iV1PHlYhMAAAAAkE/MSAEAAABAPlGkAAAAACCfKFIAAAAAkE8UKQAAAADIJ4pUEZs6daqqVasmDw8PhYWFaePGjf+6/YIFC1S3bl15eHgoJCREy5Yty7XeMAyNHj1agYGB8vT0VEREhA4ePFiUL8Hu5WeMP/nkE912220qV66cypUrp4iIiCu2HzBggCwWS65bhw4divpl2K38jO+sWbOuGDsPD49c23AM55af8W3btu0V42uxWNS5c2fbNhy/OX799Vd16dJFQUFBslgsWrRo0XUfs2bNGjVt2lTu7u665ZZbNGvWrCu2ye/vdEeV3/H97rvvdPfdd6tSpUry9fVVeHi4Vq5cmWubsWPHXnHs1q1btwhfhf3K7/iuWbPmqr8b4uPjc23H8fu3/I7x1X63WiwWNWjQwLYNx3COCRMmqEWLFvLx8VHlypXVrVs37d+//7qPK4nvgSlSRejrr7/WM888ozFjxmjr1q0KDQ1VZGSkTp48edXtN2zYoL59+2rgwIHatm2bunXrpm7dumnXrl22bd566y1NmjRJ06dPV0xMjLy9vRUZGan09PTiell2Jb9jvGbNGvXt21e//PKLoqKiFBwcrPbt2+uvv/7KtV2HDh104sQJ223u3LnF8XLsTn7HV8r5VvJ/jt2RI0dyrecY/lt+x/e7777LNba7du2Ss7Oz7r///lzbcfxKaWlpCg0N1dSpU/O0fWxsrDp37qw777xT27dv14gRIzRo0KBcb/YL8vfBUeV3fH/99VfdfffdWrZsmbZs2aI777xTXbp00bZt23Jt16BBg1zH7m+//VYU8e1efsf3sv379+cav8qVK9vWcfzmlt8xnjhxYq6xjYuLU/ny5a/4/csxLK1du1ZDhgxRdHS0Vq1apaysLLVv315paWnXfEyJfQ9soMi0bNnSGDJkiO1+dna2ERQUZEyYMOGq2/fq1cvo3LlzrmVhYWHGY489ZhiGYVitViMgIMB4++23besTExMNd3d3Y+7cuUXwCuxffsf4f128eNHw8fExPv/8c9uy/v37G127di3sqCVSfsd35syZhp+f3zX3xzGc240ev++//77h4+NjpKam2pZx/F5JkrFw4cJ/3ea5554zGjRokGtZ7969jcjISNv9G/3zclR5Gd+rqV+/vjFu3Djb/TFjxhihoaGFF8xB5GV8f/nlF0OSce7cuWtuw/F7bQU5hhcuXGhYLBbjzz//tC3jGL66kydPGpKMtWvXXnObkvoemBmpIpKZmaktW7YoIiLCtszJyUkRERGKioq66mOioqJybS9JkZGRtu1jY2MVHx+faxs/Pz+FhYVdc5+OrCBj/L/Onz+vrKwslS9fPtfyNWvWqHLlyqpTp46eeOIJnTlzplCzlwQFHd/U1FRVrVpVwcHB6tq1q3bv3m1bxzH8t8I4fmfMmKE+ffrI29s713KO3/y73u/fwvjzwt+sVqtSUlKu+N178OBBBQUFqUaNGurXr5+OHj1qUsKSqXHjxgoMDNTdd9+t9evX25Zz/Ba+GTNmKCIiQlWrVs21nGP4SklJSZJ0xd/3fyqp74EpUkXk9OnTys7Olr+/f67l/v7+V3xm+bL4+Ph/3f7yf/OzT0dWkDH+X88//7yCgoJy/cXs0KGDZs+erdWrV+vNN9/U2rVr1bFjR2VnZxdqfntXkPGtU6eOPvvsMy1evFhffvmlrFarWrdurWPHjkniGP6nGz1+N27cqF27dmnQoEG5lnP8Fsy1fv8mJyfrwoULhfL7Bn975513lJqaql69etmWhYWFadasWVqxYoWmTZum2NhY3XbbbUpJSTExackQGBio6dOn69tvv9W3336r4OBgtW3bVlu3bpVUOP+/xN+OHz+u5cuXX/H7l2P4SlarVSNGjFCbNm3UsGHDa25XUt8Du5j2zIDJ3njjDc2bN09r1qzJdUGEPn362H4OCQlRo0aNVLNmTa1Zs0bt2rUzI2qJER4ervDwcNv91q1bq169evroo480fvx4E5M5nhkzZigkJEQtW7bMtZzjF/Zuzpw5GjdunBYvXpzrHJ6OHTvafm7UqJHCwsJUtWpVzZ8/XwMHDjQjaolRp04d1alTx3a/devWOnz4sN5//3198cUXJiZzTJ9//rnKli2rbt265VrOMXylIUOGaNeuXQ57rhgzUkWkYsWKcnZ2VkJCQq7lCQkJCggIuOpjAgIC/nX7y//Nzz4dWUHG+LJ33nlHb7zxhn788Uc1atToX7etUaOGKlasqEOHDt1w5pLkRsb3MldXVzVp0sQ2dhzDf7uR8U1LS9O8efPy9D/m0nr85te1fv/6+vrK09OzUP4+QJo3b54GDRqk+fPnX/Exnv9VtmxZ1a5dm2O3gFq2bGkbO47fwmMYhj777DM9+OCDcnNz+9dtS/sxPHToUP3www/65ZdfVKVKlX/dtqS+B6ZIFRE3Nzc1a9ZMq1evti2zWq1avXp1rn+x/6fw8PBc20vSqlWrbNtXr15dAQEBubZJTk5WTEzMNffpyAoyxlLOVV/Gjx+vFStWqHnz5td9nmPHjunMmTMKDAwslNwlRUHH95+ys7O1c+dO29hxDP/tRsZ3wYIFysjI0AMPPHDd5ymtx29+Xe/3b2H8fSjt5s6dq4cfflhz587Ndcn+a0lNTdXhw4c5dgto+/bttrHj+C08a9eu1aFDh/L0D1ml9Rg2DENDhw7VwoUL9fPPP6t69erXfUyJfQ9s2mUuSoF58+YZ7u7uxqxZs4w9e/YYgwcPNsqWLWvEx8cbhmEYDz74oPHCCy/Ytl+/fr3h4uJivPPOO8bevXuNMWPGGK6ursbOnTtt27zxxhtG2bJljcWLFxu///670bVrV6N69erGhQsXiv312YP8jvEbb7xhuLm5Gd98841x4sQJ2y0lJcUwDMNISUkxRo4caURFRRmxsbHGTz/9ZDRt2tSoVauWkZ6ebsprNFN+x3fcuHHGypUrjcOHDxtbtmwx+vTpY3h4eBi7d++2bcMx/Lf8ju9lt956q9G7d+8rlnP8/i0lJcXYtm2bsW3bNkOS8d577xnbtm0zjhw5YhiGYbzwwgvGgw8+aNv+jz/+MLy8vIz/+7//M/bu3WtMnTrVcHZ2NlasWGHb5np/XqVJfsf3q6++MlxcXIypU6fm+t2bmJho2+bZZ5811qxZY8TGxhrr1683IiIijIoVKxonT54s9tdntvyO7/vvv28sWrTIOHjwoLFz505j+PDhhpOTk/HTTz/ZtuH4zS2/Y3zZAw88YISFhV11nxzDOZ544gnDz8/PWLNmTa6/7+fPn7dt4yjvgSlSRWzy5MnGzTffbLi5uRktW7Y0oqOjbevuuOMOo3///rm2nz9/vlG7dm3Dzc3NaNCggbF06dJc661WqzFq1CjD39/fcHd3N9q1a2fs37+/OF6K3crPGFetWtWQdMVtzJgxhmEYxvnz54327dsblSpVMlxdXY2qVasajz76aKn9H41h5G98R4wYYdvW39/f6NSpk7F169Zc++MYzi2/vyP27dtnSDJ+/PHHK/bF8fu3y5eD/t/b5fHs37+/cccdd1zxmMaNGxtubm5GjRo1jJkzZ16x33/78ypN8ju+d9xxx79ubxg5l5sPDAw03NzcjJtuusno3bu3cejQoeJ9YXYiv+P75ptvGjVr1jQ8PDyM8uXLG23btjV+/vnnK/bL8fu3gvyOSExMNDw9PY2PP/74qvvkGM5xtXGVlOt3qqO8B7YYhmEU2XQXAAAAADggzpECAAAAgHyiSAEAAABAPlGkAAAAACCfKFIAAAAAkE8UKQAAAADIJ4oUAAAAAOQTRQoAAAAA8okiBQDADbBYLFq0aJHZMQAAxYwiBQAosQYMGCCLxXLFrUOHDmZHAwA4OBezAwAAcCM6dOigmTNn5lrm7u5uUhoAQGnBjBQAoERzd3dXQEBArlu5cuUk5Xzsbtq0aerYsaM8PT1Vo0YNffPNN7kev3PnTt11113y9PRUhQoVNHjwYKWmpuba5rPPPlODBg3k7u6uwMBADR06NNf606dPq3v37vLy8lKtWrW0ZMmSon3RAADTUaQAAA5t1KhR6tmzp3bs2KF+/fqpT58+2rt3ryQpLS1NkZGRKleunDZt2qQFCxbop59+ylWUpk2bpiFDhmjw4MHauXOnlixZoltuuSXXc4wbN069evXS77//rk6dOqlfv346e/Zssb5OAEDxshiGYZgdAgCAghgwYIC+/PJLeXh45Fr+3//+V//9739lsVj0+OOPa9q0abZ1rVq1UtOmTfXhhx/qk08+0fPPP6+4uDh5e3tLkpYtW6YuXbro+PHj8vf310033aSHH35Yr7766lUzWCwWvfzyyxo/fryknHJWpkwZLV++nHO1AMCBcY4UAKBEu/POO3MVJUkqX7687efw8PBc68LDw7V9+3ZJ0t69exUaGmorUZLUpk0bWa1W7d+/XxaLRcePH1e7du3+NUOjRo1sP3t7e8vX11cnT54s6EsCAJQAFCkAQInm7e19xUftCounp2eetnN1dc1132KxyGq1FkUkAICd4BwpAIBDi46OvuJ+vXr1JEn16tXTjh07lJaWZlu/fv16OTk5qU6dOvLx8VG1atW0evXqYs0MALB/zEgBAEq0jIwMxcfH51rm4uKiihUrSpIWLFig5s2b69Zbb9VXX32ljRs3asaMGZKkfv36acyYMerfv7/Gjh2rU6dOadiwYXrwwQfl7+8vSRo7dqwef/xxVa5cWR07dlRKSorWr1+vYcOGFe8LBQDYFYoUAKBEW7FihQIDA3Mtq1Onjvbt2ycp54p68+bN05NPPqnAwEDNnTtX9evXlyR5eXlp5cqVGj58uFq0aCEvLy/17NlT7733nm1f/fv3V3p6ut5//32NHDlSFStW1H333Vd8LxAAYJe4ah8AwGFZLBYtXLhQ3bp1MzsKAMDBcI4UAAAAAOQTRQoAAAAA8olzpAAADotPrwMAigozUgAAAACQTxQpAAAAAMgnihQAAAAA5BNFCgAAAADyiSIFAAAAAPlEkQIAAACAfKJIAQAAAEA+UaQAAAAAIJ8oUgAAAACQT/8Pw8n69R5fzh0AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
